{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3749ed5",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3facc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load Libraries\n",
    "from pandas.api.indexers import BaseIndexer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as txt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings # this allows us to better control warning messages\n",
    "\n",
    "# Set matplotlib to output plots without having to use plt.show()\n",
    "%matplotlib inline \n",
    "\n",
    "# Set max row display\n",
    "#pd.set_option('display.max_row', 200)\n",
    "\n",
    "# Set iPython's max column width to 60\n",
    "#pd.set_option('display.max_columns', 60)\n",
    "\n",
    "# Use Seaborn Settings for all plots\n",
    "# Adjust figure size\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "\n",
    "# Adjust font size for seaborn graphs and configure settings\n",
    "sns.set()\n",
    "\n",
    "# Displays all columns of each DataFrame when the entire DataFrame is called\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Do not print warning messages\n",
    "#warnings.simplefilter('ignore')\n",
    "\n",
    "DH_plus_dist=pd.read_csv(\"DH_plus_dist_interpolated.csv\")\n",
    "LA_plus_dist=pd.read_csv(\"LA_plus_dist_interpolated.csv\")\n",
    "Lucy_16_20_May_plus_dist=pd.read_csv(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31297c-cf4a-4730-b90e-0a2aef1d1310",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe3a28-c77a-44a2-b9f2-e46147f75409",
   "metadata": {},
   "source": [
    "#### Custom Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caac525-3df6-49f8-8282-cc19b250e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomIndexer(BaseIndexer):\n",
    "        \n",
    "    # From 1.5.0 onwards step is a mandatory parameter. Adding a default value allows for backward and foreward compatibility up to the latest version 2.2.3. Currently using 1.4.4.\n",
    "    # All rolling windows are centered reguardless of center parameter.\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        \n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        splitDatasets = []\n",
    "\n",
    "        print(\"Custom indexer number of values:\",num_values)\n",
    "        print(\"Indexer fixed window size (inherited parameter with immutable value and is not used):\",self.window_size)\n",
    "        print(\"Indexer dynamic window size (custom parameter with mutable value and is used):\",self.dynamic_window_size)\n",
    "\n",
    "        # For an even window size you can only have integer indexes. Hence \"middle\" index value is rounded up automatically to the nearest integer. Thus the \"middle\" index closer to the right value.\n",
    "        # Within the BaseIndexer class the start and end bounds work just like the range(a, b) function where: \n",
    "        # a = the starting index or value.\n",
    "        # b = the index or value up to but not including.  \n",
    "        if self.dynamic_window_size % 2 == 0:\n",
    "            minShift = self.dynamic_window_size/2\n",
    "            maxShift = self.dynamic_window_size/2\n",
    "        else:\n",
    "            minShift = int(self.dynamic_window_size/2)\n",
    "            maxShift = round(self.dynamic_window_size/2)\n",
    "            \n",
    "\n",
    "        for i in range(num_values):\n",
    "            # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "            # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "            if i - minShift < 0:\n",
    "                start[i] = 0\n",
    "                end[i] = i + maxShift\n",
    "\n",
    "            elif i + maxShift > num_values - 1:\n",
    "                start[i] = i - minShift\n",
    "                end[i] = num_values \n",
    "\n",
    "            else:\n",
    "                start[i] = i - minShift\n",
    "                end[i] = i + maxShift\n",
    "        #These print statements were to check if the indexes were assigned to the correct bounds\n",
    "        #print(start)\n",
    "        #print(end)\n",
    "\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551da675-4a8a-47b7-9f6f-e5dfe642f8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_datetimes(dataset):\n",
    "    print(\"Number of duplicate records found and removed:\",dataset.index.duplicated().sum())\n",
    "    # transform() method used so the newly formed groupded DataFrame retains its index structure and dimensions meaning records can be added back to the original DataFrame easily.\n",
    "    x = dataset.groupby(dataset.index,sort=False)['distance'].transform('mean')\n",
    "    #Assign duplicate DateTimes with the the mean of all of the distances to the dataset\n",
    "    dataset.loc[:,'distance'] = x.loc[:]\n",
    "    #Drop duplicate records.\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    return dataset\n",
    "    # For potential future testing\n",
    "    # 1. Count initital number of records.\n",
    "    # 2. Count number of duplicates within inital DataFrame.\n",
    "    # 3. Remove dupliucates.\n",
    "    # 4. Difference between number of records between before and after removing duplicates.\n",
    "    # 5. Does difference equal the amount of duplicates found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c0d2d-3156-4ba6-855b-7b4773349a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regular hourly average and standard deviation distance and centred rolling window average and standard deviation distance\n",
    "\n",
    "def hourly_summary(dataset):\n",
    "\n",
    "    hourly_dataset = dataset.groupby([dataset.index.date, dataset.index.hour])\n",
    "\n",
    "    # agg() function because multiple summaries are being caluclated, and creates a DataFrame with a different (Multiindex) index structure and dimensions from the orginal.\n",
    "    hourly_dataset = hourly_dataset.agg({'distance':['mean','std']})\n",
    "    \n",
    "    #hourly mean and std columns added to the original dataFrame with the Dataset on each observed record\n",
    "    hourly_mean = dataset.groupby([dataset.index.date,dataset.index.hour])[['distance']].transform('mean')\n",
    "    hourly_standard_deviation = dataset.groupby([dataset.index.date,dataset.index.hour])[['distance']].transform('std')\n",
    "    dataset['hourly_mean'] = hourly_mean['distance']\n",
    "    dataset['hourly_standard_deviation'] = hourly_standard_deviation['distance']\n",
    "\n",
    "    \n",
    "    rolling_hour_mean = dataset.loc[:,'distance'].rolling('1h',center=True).mean()\n",
    "    rolling_hour_standard_deviation = dataset.loc[:,'distance'].rolling('1h',center=True).std()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hour_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hour_standard_deviation\n",
    "\n",
    "    #Returns orginal DataFrame and new hourly_dataset DataFrame indexed by date and hour only, with repeated hourly average recordings removed. Different dimensions means noit all data could be aggregated onto the original DataFrame. \n",
    "    return dataset, hourly_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee6b9c-12d9-49d2-b6d3-f0a80ba2d583",
   "metadata": {},
   "source": [
    "#### Display Time Series Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725c191-5086-41b2-a5bb-6d9dcaa37ff8",
   "metadata": {},
   "source": [
    "NOTE: If I wanted to plot only time on the x-axis from the Timestamp datatype: the time attribute was returning the time object, what I needed to do was call the time() method to return the time value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5a6d5-3a51-49a7-b921-3003944b497b",
   "metadata": {},
   "source": [
    "Without Y axis range from 0 to largest distance value. Range from smallest to largest recorded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858266fd-f02d-425f-8d55-604d1fc8f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def satisfied_condition_record_count(dataset):\n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "    total_true_records = 0\n",
    "    total_false_records = 0\n",
    "    for date in dates:\n",
    "        condition = dataset.sample_date==date\n",
    "        if \"satisfied_filtering_condition\" in dataset.columns:\n",
    "            distances = dataset.loc[condition,['distance','satisfied_filtering_condition']]\n",
    "            green_datetimes = distances.loc[distances.satisfied_filtering_condition == True,:]\n",
    "            red_datetimes = distances.loc[distances.satisfied_filtering_condition == False,:]\n",
    "            print(\"Within\",date)\n",
    "            print(\"Number of records satisfying filtering condition:\",len(green_datetimes))\n",
    "            print(\"Number of records not satisfying filtering condition:\",len(red_datetimes))\n",
    "            total_true_records += green_datetimes.count()\n",
    "            total_false_records += red_datetimes.count()\n",
    "        \n",
    "    print(\"Total number of records satisfying filtering condition:\",total_true_records)\n",
    "    print(\"Total of records not satisfying filtering condition:\",total_false_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc25711-49a2-49b6-b7e7-8e08dbd59165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relative_range_time_series_line(dataset, *other_column_names): \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "    print(dates)\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        print(min)\n",
    "        print(max)\n",
    "        condition = dataset.sample_date==date\n",
    "        times = dataset.loc[condition,'sample_time']\n",
    "        datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "        distances = dataset.loc[condition,'distance']\n",
    "        axis[i].set_xlim(min,max)\n",
    "        axis[i].plot(datetimes,distances,color='g')\n",
    "        for arg in other_column_names:\n",
    "            if arg in dataset.columns:\n",
    "                axis[i].plot(datetimes, dataset.loc[condition,arg],'--')\n",
    "        #if 'hourly_mean' in dataset.columns:\n",
    "            # Compare against hourly average.\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'hourly_mean'],'--')\n",
    "            # Compare against moving hourly average.\n",
    "            # axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly_mean'],'--')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddd609-f4e6-4d3b-8c95-3d0330ef042e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relative_range_time_series_plot(dataset, *other_column_names):\n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "    print(dates)\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "        times = dataset.loc[condition,'sample_time']\n",
    "        datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "        axis[i].set_xlim(min,max)\n",
    "        # Filterning condition shown only for plots instead of lines as it is more intuative.\n",
    "        if \"satisfied_filtering_condition\" in dataset.columns:\n",
    "            distances = dataset.loc[condition,['distance','satisfied_filtering_condition']]\n",
    "            green_datetimes = distances.loc[distances.satisfied_filtering_condition == True,:]\n",
    "            red_datetimes = distances.loc[distances.satisfied_filtering_condition == False,:]\n",
    "            axis[i].plot(green_datetimes.index,green_datetimes.distance,'x',markersize=3,color='g')\n",
    "            axis[i].plot(red_datetimes.index,red_datetimes.distance,'x',markersize=3,color='r')\n",
    "        else:\n",
    "            distances = dataset.loc[condition,'distance']\n",
    "            axis[i].plot(datetimes,distances,'x',markersize=3,color='g')\n",
    "        for arg in other_column_names:\n",
    "            if arg in dataset.columns:\n",
    "                axis[i].plot(datetimes, dataset.loc[condition,arg],'--')\n",
    "        #if 'hourly_mean' in dataset.columns:\n",
    "            # Compare against hourly average.\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'hourly_mean'],'--')\n",
    "            # Compare against moving hourly average.\n",
    "            # axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly_mean'],'--')\n",
    "        #if 'rolling_hourly_mean' in dataset.columns:\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly_mean'],'--')\n",
    "        #if 'rolling_hourly_standard_deviation' in dataset.columns:\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly_standard_deviation'],'--')\n",
    "        #if other_column_name in dataset.columns:\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,other_column_name],'--')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fa190-722c-445a-934d-b9dd89f3fb1b",
   "metadata": {},
   "source": [
    "With Y axis range from 0 to largest distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead8274-27b1-4ce8-a7e1-36efb794e22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def absolute_range_time_series_line(dataset, *other_column_names):    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "    largestY = dataset.loc[:,'distance'].max()\n",
    "    print(\"Largest distance value:\",largestY)\n",
    "    print(dates)\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "        times = dataset.loc[condition,'sample_time']\n",
    "        datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "        distances = dataset.loc[condition,'distance']\n",
    "        axis[i].set_xlim(min,max)\n",
    "        axis[i].set_ylim(0,largestY)\n",
    "        axis[i].plot(datetimes,distances,color='g')\n",
    "        for arg in other_column_names:\n",
    "            if arg in dataset.columns:\n",
    "                axis[i].plot(datetimes, dataset.loc[condition,arg],'--')\n",
    "        #if 'hourly_mean' in dataset.columns:\n",
    "            # Compare against hourly average.\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'hourly_mean'],'--')\n",
    "            # Compare against moving hourly average.\n",
    "            # axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly+mean'],'--')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991130d4-cec4-4564-97e3-2fbefb9d17e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def absolute_range_time_series_plot(dataset, *other_column_names):\n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "    largestY = dataset.loc[:,'distance'].max()\n",
    "    print(\"Largest distance value:\",largestY)\n",
    "    print(dates)\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "        times = dataset.loc[condition,'sample_time']\n",
    "        datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "        axis[i].set_xlim(min,max)\n",
    "        axis[i].set_ylim(0,largestY)\n",
    "        # Filterning condition shown only for plots instead of lines as it is more intuative.\n",
    "        if \"satisfied_filtering_condition\" in dataset.columns:\n",
    "            distances = dataset.loc[condition,['distance','satisfied_filtering_condition']]\n",
    "            green_datetimes = distances.loc[distances.satisfied_filtering_condition == True,:]\n",
    "            red_datetimes = distances.loc[distances.satisfied_filtering_condition == False,:]\n",
    "            axis[i].plot(green_datetimes.index,green_datetimes.distance,'x',markersize=3,color='g')\n",
    "            axis[i].plot(red_datetimes.index,red_datetimes.distance,'x',markersize=3,color='r')\n",
    "        else:\n",
    "            distances = dataset.loc[condition,'distance']\n",
    "            axis[i].plot(datetimes,distances,'x',markersize=3,color='g')\n",
    "        for arg in other_column_names:\n",
    "            if arg in dataset.columns:\n",
    "                axis[i].plot(datetimes, dataset.loc[condition,arg],'--')\n",
    "        #if 'hourly_mean' in dataset.columns:\n",
    "            # Compare against hourly average.\n",
    "            #axis[i].plot(datetimes, dataset.loc[condition,'hourly_mean'],'--')\n",
    "            # Compare against moving hourly average.\n",
    "            # axis[i].plot(datetimes, dataset.loc[condition,'rolling_hourly_mean'],'--')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc730b-c7d6-4b4f-b2e4-895566d8ce8d",
   "metadata": {},
   "source": [
    "#### Time Series Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded15a5-9389-4823-bf60-d31f0bf2e8a0",
   "metadata": {},
   "source": [
    "For plotting clusters of different distance values on the same graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bab01-7f4e-4c81-a56f-a2c3ba6cfda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inertia_elbow_method_of_whole_duration(dataset):\n",
    "    \n",
    "    inertias = []\n",
    "    \n",
    "    number_of_clusters = range(1,21)\n",
    "    \n",
    "    raw_values = dataset.distance\n",
    "    \n",
    "    if \"five_record_mean\" in dataset.columns:\n",
    "        five_record_values = dataset.five_record_mean\n",
    "    else:\n",
    "        five_record_values = dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "        \n",
    "    if \"twenty_five_record_mean\" in dataset.columns:\n",
    "        twenty_five_record_values = dataset.twenty_five_record_mean\n",
    "    else:\n",
    "        twenty_five_record_values = dataset.loc[:,'distance'].rolling(25,center=True).mean()\n",
    "    \n",
    "    #print(type(str(raw_values.index.array)))\n",
    "    \n",
    "    #datetimes = raw_values.index.array.strftime('%d/%m/%y %H:%M:%S')\n",
    "    #print(type(datetimes[0]))\n",
    "    #print(datetimes[0])\n",
    "    \n",
    "    ## Convert Datetime indexes to integers in the form of Unix epochs (non leap year seconds since 01/01/1970) as k-means clustering only compares against numeric values.\n",
    "    raw_unix_epoch = (raw_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "    five_record_unix_epoch = (five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "    twenty_five_record_unix_epoch = (twenty_five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    \n",
    "    all_data_points = list(zip(raw_unix_epoch.array, raw_values.array)) + list(zip(five_record_unix_epoch.array, five_record_values.array)) + list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "    \n",
    "    \n",
    "    #five_record_data_points = list(zip(five_record_unix_epoch.array, five_record_values.array))\n",
    "    \n",
    "    #twenty_five_record_data_points = list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "\n",
    "    number_of_values = len(raw_values) + len(five_record_values) + len(twenty_five_record_values)\n",
    "    for i in number_of_clusters:\n",
    "        \n",
    "            kmeans = KMeans(n_clusters=i)\n",
    "        \n",
    "            kmeans.fit(all_data_points)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(number_of_clusters, inertias, marker='o')\n",
    "    plt.title('Elbow method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d9a13-521d-4663-8d99-2475d4d15bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def k_means_clustering_of_whole_duration(dataset, num_of_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_of_clusters)\n",
    "    \n",
    "    unix_epoch = (dataset.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "    all_data_points = np.array(list(zip(unix_epoch.array, dataset.distance.array)) + list(zip(unix_epoch.array, dataset.five_record_mean.array)) + list(zip(unix_epoch.array, dataset.twenty_five_record_mean.array)))\n",
    "    \n",
    "    kmeans.fit(all_data_points)\n",
    "    \n",
    "    # save new clusters for chart \n",
    "    y_km = kmeans.fit_predict(all_data_points)\n",
    "    \n",
    "    plt.scatter(all_data_points[:,0], all_data_points[:,1], c=y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d05cf-f0f9-41e3-b98d-d35c5a1e088a",
   "metadata": {},
   "source": [
    "Attempt to apply elbow method to each day of the provided dataset on seperate graphs. Time constraints held this back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50c296-15d2-4787-97c6-210a8935a166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Attempt to apply elbow method to each day of the provided dataset on seperate graphs. Time constraints held this back.\n",
    "\n",
    "#def inertia_elbow_method_of_daily_duration(dataset):\n",
    "    \n",
    "#    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "#    figure, axis = plt.subplots(dates.size)\n",
    "#    i = 0\n",
    "#    for date in dates:\n",
    "#        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "#        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "#        condition = dataset.sample_date==date\n",
    "\n",
    "#        axis[i].set_xlim(min,max)\n",
    "        # Filterning condition shown only for plots instead of lines as it is more intuative.\n",
    "        \n",
    "#        raw_unix_epoch = (raw_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "#        five_record_unix_epoch = (five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "#        twenty_five_record_unix_epoch = (twenty_five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    \n",
    "#        all_data_points = list(zip(raw_unix_epoch.array, raw_values.array)) + list(zip(five_record_unix_epoch.array, five_record_values.array)) + list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "    \n",
    "    \n",
    "        #five_record_data_points = list(zip(five_record_unix_epoch.array, five_record_values.array))\n",
    "    \n",
    "        #twenty_five_record_data_points = list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "\n",
    "#        number_of_values = len(raw_values) + len(five_record_values) + len(twenty_five_record_values)\n",
    "#        for i in number_of_clusters:\n",
    "        \n",
    "#            kmeans = KMeans(n_clusters=i)\n",
    "        \n",
    "#            kmeans.fit(all_data_points)\n",
    "#            inertias.append(kmeans.inertia_)\n",
    "\n",
    "#        plt.plot(number_of_clusters, inertias, marker='o')\n",
    "#        plt.title('Elbow method')\n",
    "#        plt.xlabel('Number of clusters')\n",
    "#        plt.ylabel('Inertia')\n",
    "#        plt.show()\n",
    "        \n",
    "        \n",
    "#        if \"satisfied_filtering_condition\" in dataset.columns:\n",
    "#            distances = dataset.loc[condition,['distance','satisfied_filtering_condition']]\n",
    "#            green_datetimes = distances.loc[distances.satisfied_filtering_condition == True,:]\n",
    "#            red_datetimes = distances.loc[distances.satisfied_filtering_condition == False,:]\n",
    "\n",
    "#            axis[i].plot(green_datetimes.index,green_datetimes.distance,'x',markersize=3,color='g')\n",
    "#            axis[i].plot(red_datetimes.index,red_datetimes.distance,'x',markersize=3,color='r')\n",
    "#        else:\n",
    "#            distances = dataset.loc[condition,'distance']\n",
    "#            axis[i].plot(datetimes,distances,'x',markersize=3,color='g')\n",
    "#        for arg in other_column_names:\n",
    "#            if arg in dataset.columns:\n",
    "#                axis[i].plot(datetimes, dataset.loc[condition,arg],'x',markersize=3)\n",
    "#        i+=1\n",
    "        \n",
    "    \n",
    "#    inertias = []\n",
    "    \n",
    "#    number_of_clusters = range(1,21)\n",
    "    \n",
    "#    raw_values = dataset.distance\n",
    "    \n",
    "#    if \"five_record_mean\" in dataset.columns:\n",
    "#        five_record_values = dataset.five_record_mean\n",
    "#    else:\n",
    "#        five_record_values = dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "        \n",
    "#    if \"twenty_five_record_mean\" in dataset.columns:\n",
    "#        twenty_five_record_values = dataset.twenty_five_record_mean\n",
    "#    else:\n",
    "#        twenty_five_record_values = dataset.loc[:,'distance'].rolling(25,center=True).mean()\n",
    "    \n",
    "    #print(type(str(raw_values.index.array)))\n",
    "    \n",
    "    #datetimes = raw_values.index.array.strftime('%d/%m/%y %H:%M:%S')\n",
    "    #print(type(datetimes[0]))\n",
    "    #print(datetimes[0])\n",
    "    \n",
    "    ## Convert Datetime indexes to integers in the form of Unix epochs (non leap year seconds since 01/01/1970) as k-means clustering only compares against numeric values.\n",
    "#    raw_unix_epoch = (raw_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "#    five_record_unix_epoch = (five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "#    twenty_five_record_unix_epoch = (twenty_five_record_values.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    \n",
    "#    all_data_points = list(zip(raw_unix_epoch.array, raw_values.array)) + list(zip(five_record_unix_epoch.array, five_record_values.array)) + list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "    \n",
    "    \n",
    "    #five_record_data_points = list(zip(five_record_unix_epoch.array, five_record_values.array))\n",
    "    \n",
    "    #twenty_five_record_data_points = list(zip(twenty_five_record_unix_epoch.array, twenty_five_record_values.array))\n",
    "\n",
    "#    number_of_values = len(raw_values) + len(five_record_values) + len(twenty_five_record_values)\n",
    "#    for i in number_of_clusters:\n",
    "        \n",
    "#            kmeans = KMeans(n_clusters=i)\n",
    "        \n",
    "#            kmeans.fit(all_data_points)\n",
    "#            inertias.append(kmeans.inertia_)\n",
    "\n",
    "#    plt.plot(number_of_clusters, inertias, marker='o')\n",
    "#    plt.title('Elbow method')\n",
    "#    plt.xlabel('Number of clusters')\n",
    "#    plt.ylabel('Inertia')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181b968-eb56-488b-b0f6-ba24d12469eb",
   "metadata": {},
   "source": [
    "Attempt to apply clustering to each day of the provided dataset on seperate graphs. Time constraints held this back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39829ff7-3a30-434a-91de-03f4aa309584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to apply clustering to each day of the provided dataset on seperate graphs. Time constraints held this back.\n",
    "\n",
    "#def k_means_clustering_of_daily_duration(dataset, num_of_clusters):\n",
    "    \n",
    "#    dates = dataset.loc[:,'sample_date'].unique()\n",
    "#    print(dates)\n",
    "#    figure, axis = plt.subplots(dates.size)\n",
    "#    i = 0\n",
    "#    for date in dates:\n",
    "#        kmeans = KMeans(n_clusters=num_of_clusters)\n",
    "        \n",
    "#        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True) - pd.Timestamp(\"1970-01-01\") // pd.Timedelta(\"1s\")\n",
    "#        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True) - pd.Timestamp(\"1970-01-01\") // pd.Timedelta(\"1s\")\n",
    "#        condition = dataset.sample_date==date\n",
    "#        times = dataset.loc[condition,'sample_time']\n",
    "#        datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "#        axis[i].set_xlim(min,max)\n",
    "\n",
    "#        distances = dataset.loc[condition,'distance']\n",
    "#        five_record_means = dataset.loc[condition,'five_record_mean']\n",
    "#        twenty_five_record_means = dataset.loc[condition,'twenty_five_record_mean']\n",
    "        \n",
    "#        unix_epoch = (times.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "    \n",
    "#        all_data_points = np.array(list(zip(unix_epoch.array, distances.distance.array)) + list(zip(unix_epoch.array, five_record_means.five_record_mean.array)) + list(zip(unix_epoch.array, twenty_five_record_means.twenty_five_record_mean.array)))\n",
    "        \n",
    "#        kmeans.fit(all_data_points)\n",
    "    \n",
    "        # save new clusters for chart \n",
    "#        y_km = kmeans.fit_predict(all_data_points)\n",
    "        \n",
    "#        axis[i].scatter(all_data_points[:,0], all_data_points[:,1], c=y_km)\n",
    "\n",
    "#        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705decb-5342-4c4d-ae84-032cb5281f27",
   "metadata": {},
   "source": [
    "#### Rolling Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797018f-441d-42da-85a5-e98d2dd7c71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Old function\n",
    "\n",
    "# def midpoints(window_size,dataframe):\n",
    "    \n",
    "    # Assuming all rolling windows are centered.\n",
    "    # Because midpoint index must be set to an integer value, if the window size is even it needs to be rounded up to the nearest integer.\n",
    "#    minShift = round(window_size/2)\n",
    "#    maxShift = int(window_size/2)\n",
    "#    indexes = dataframe.index # or indexes = temp.index\n",
    "#    newIndexes = []\n",
    "#    count = 0\n",
    "\n",
    "    # This is to check the positon of the indexes\n",
    "#    for i in range(indexes.size):\n",
    "\n",
    "        # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "        # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "#        if i - minShift < 0:\n",
    "#            earliest = indexes[0]\n",
    "#            latest = indexes[i+maxShift]\n",
    "#        elif i + maxShift > indexes.size-1:\n",
    "#            earliest = indexes[i-minShift]\n",
    "#            latest = indexes[indexes.size-1]\n",
    "#        else:\n",
    "#            earliest = indexes[i-minShift]\n",
    "#            latest = indexes[i+maxShift]\n",
    "\n",
    "#        midpoint = pd.Interval(earliest, latest, closed='both').mid\n",
    "#        newIndexes.append(midpoint)\n",
    "    \n",
    "#    return newIndexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ef42c-9ca9-4381-9982-1f44dccb6eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def midpoints(window_size,dataframe):\n",
    "    \n",
    "    # Assuming all rolling windows are centered.\n",
    "\n",
    "    # For an even window size you can only have integer indexes. Hence \"middle\" index value is rounded up automatically to the nearest integer. Thus the \"middle\" index closer to the right value.\n",
    "    # An odd window size will always return the exact \"middle\" index between the window range. Hence shame shift sizes.\n",
    "    if window_size % 2 == 0:\n",
    "        minShift = window_size/2\n",
    "        maxShift = (window_size/2) - 1\n",
    "    else:\n",
    "        minShift = int(window_size/2)\n",
    "        maxShift = int(window_size/2)\n",
    "        \n",
    "    indexes = dataframe.index # or indexes = temp.index\n",
    "    newIndexes = []\n",
    "    count = 0\n",
    "\n",
    "    # This is to check the positon of the indexes\n",
    "    for i in range(indexes.size):\n",
    "\n",
    "        # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "        # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "        if i - minShift < 0:\n",
    "            earliest = indexes[0]\n",
    "            latest = indexes[i+maxShift]\n",
    "        elif i + maxShift > indexes.size-1:\n",
    "            earliest = indexes[i-minShift]\n",
    "            latest = indexes[indexes.size-1]\n",
    "        else:\n",
    "            earliest = indexes[i-minShift]\n",
    "            latest = indexes[i+maxShift]\n",
    "\n",
    "        midpoint = pd.Interval(earliest, latest, closed='both').mid\n",
    "        newIndexes.append(midpoint)\n",
    "    \n",
    "    return newIndexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48b871-3c34-43c6-9a1d-7dfa702a0d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistical_summary(dataset):\n",
    "\n",
    "    # The RolingWindow object automatically checks for if the Window size exceeds the min number of records per window by throwing an exception.\n",
    "\n",
    "    #Can also do it this way.\n",
    "    #DH_minute_rolling_window_summary = pd.DataFrame(index=dataset.index, data = {'mean':dataset.loc[:,'distance'].rolling('min').mean(),'std':dataset.loc[:,'distance'].rolling('min').std(),'var':dataset.loc[:,'distance'].rolling('min').var()})\n",
    "    rolling_window_summary = pd.DataFrame()\n",
    "    rolling_window_summary.loc[:,'difference'] = dataset.loc[:,'distance'].diff()\n",
    "    rolling_window_summary.loc[:,'fractional_change'] = dataset.loc[:,'distance'].pct_change()\n",
    "    \n",
    "    #rolling_window_summary.loc[:,'three_record_midpoint'] = midpoints(3, dataset)\n",
    "    #rolling_window_summary.loc[:,'one_minute_mean'] = dataset.loc[:,'distance'].rolling('1min',center=True).mean()\n",
    "    #rolling_window_summary.loc[:,'one_minute_standard_deviation'] = dataset.loc[:,'distance'].rolling('1min',center=True).std()\n",
    "    #rolling_window_summary.loc[:,'one_minute_variance'] = dataset.loc[:,'distance'].rolling('1min',center=True).var()\n",
    "    #rolling_window_summary.loc[:,'one_minute_difference'] = dataset.loc[:,'distance'].rolling('1min',center=True).mean().diff()\n",
    "    #rolling_window_summary.loc[:,'one_minute_fractional_change'] = dataset.loc[:,'distance'].rolling('1min',center=True).mean().pct_change()\n",
    "\n",
    "    #rolling_window_summary.loc[:,'five_record_midpoint'] = midpoints(5, dataset)\n",
    "    #rolling_window_summary.loc[:,'three_minute_mean'] = dataset.loc[:,'distance'].rolling('3min',center=True).mean()\n",
    "    #rolling_window_summary.loc[:,'three_minute_standard_deviation'] = dataset.loc[:,'distance'].rolling('3min',center=True).std()\n",
    "    #rolling_window_summary.loc[:,'three_minute_variance'] = dataset.loc[:,'distance'].rolling('3min',center=True).var()\n",
    "    #rolling_window_summary.loc[:,'three_minute_difference'] = dataset.loc[:,'distance'].rolling('3min',center=True).mean().diff()\n",
    "    #rolling_window_summary.loc[:,'three_minute_fractional_change'] = dataset.loc[:,'distance'].rolling('3min',center=True).mean().pct_change()\n",
    "\n",
    "    #rolling_window_summary.loc[:,'ten_record_midpoint'] = midpoints(10, dataset)\n",
    "    #rolling_window_summary.loc[:,'five_minute_mean'] = dataset.loc[:,'distance'].rolling('5min',center=True).mean()\n",
    "    #rolling_window_summary.loc[:,'five_minute_standard_deviation'] = dataset.loc[:,'distance'].rolling('5min',center=True).std()\n",
    "    #rolling_window_summary.loc[:,'five_minute_variance'] = dataset.loc[:,'distance'].rolling('5min',center=True).var()\n",
    "    #rolling_window_summary.loc[:,'five_minute_difference'] = dataset.loc[:,'distance'].rolling('5min',center=True).mean().diff()\n",
    "    #rolling_window_summary.loc[:,'five_minute_fractional_change'] = dataset.loc[:,'distance'].rolling('5min',center=True).mean().pct_change()\n",
    "    \n",
    "    #rolling_window_summary.loc[:,'ten_minute_mean'] = dataset.loc[:,'distance'].rolling('10min',center=True).mean()\n",
    "    #rolling_window_summary.loc[:,'ten_minute_standard_deviation'] = dataset.loc[:,'distance'].rolling('10min',center=True).std()\n",
    "    #rolling_window_summary.loc[:,'ten_minute_variance'] = dataset.loc[:,'distance'].rolling('10min',center=True).var()\n",
    "    #rolling_window_summary.loc[:,'ten_minute_difference'] = dataset.loc[:,'distance'].rolling('10min',center=True).mean().diff()\n",
    "    #rolling_window_summary.loc[:,'ten_minute_fractional_change'] = dataset.loc[:,'distance'].rolling('10min',center=True).mean().pct_change()\n",
    "    \n",
    "    #rolling_window_summary.loc[:,'hour_mean'] = dataset.loc[:,'distance'].rolling('h',center=True).mean()\n",
    "    #rolling_window_summary.loc[:,'hour_standard_deviation'] = dataset.loc[:,'distance'].rolling('h',center=True).std()\n",
    "    #rolling_window_summary.loc[:,'hour_variance'] = dataset.loc[:,'distance'].rolling('h',center=True).var()\n",
    "    #rolling_window_summary.loc[:,'hour_difference'] = dataset.loc[:,'distance'].rolling('h',center=True).mean().diff()\n",
    "    #rolling_window_summary.loc[:,'hour_fractional_change'] = dataset.loc[:,'distance'].rolling('h',center=True).mean().pct_change()\n",
    "    \n",
    "    rolling_window_summary.loc[:,'three_record_midpoint'] = midpoints(3, dataset)\n",
    "    rolling_window_summary.loc[:,'three_record_mean'] = dataset.loc[:,'distance'].rolling(3,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'three_record_standard_deviation'] = dataset.loc[:,'distance'].rolling(3,center=True).std()\n",
    "    rolling_window_summary.loc[:,'three_record_variance'] = dataset.loc[:,'distance'].rolling(3,center=True).var()\n",
    "    rolling_window_summary.loc[:,'three_record_difference'] = dataset.loc[:,'distance'].rolling(3,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'three_record_fractional_change'] = dataset.loc[:,'distance'].rolling(3,center=True).mean().pct_change()\n",
    "\n",
    "    rolling_window_summary.loc[:,'five_record_midpoint'] = midpoints(5, dataset)\n",
    "    rolling_window_summary.loc[:,'five_record_mean'] = dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'five_record_standard_deviation'] = dataset.loc[:,'distance'].rolling(5,center=True).std()\n",
    "    rolling_window_summary.loc[:,'five_record_variance'] = dataset.loc[:,'distance'].rolling(5,center=True).var()\n",
    "    rolling_window_summary.loc[:,'five_record_difference'] = dataset.loc[:,'distance'].rolling(5,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'five_record_fractional_change'] = dataset.loc[:,'distance'].rolling(5,center=True).mean().pct_change()\n",
    "\n",
    "    rolling_window_summary.loc[:,'ten_record_midpoint'] = midpoints(10, dataset)\n",
    "    rolling_window_summary.loc[:,'ten_record_mean'] = dataset.loc[:,'distance'].rolling(10,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'ten_record_standard_deviation'] = dataset.loc[:,'distance'].rolling(10,center=True).std()\n",
    "    rolling_window_summary.loc[:,'ten_record_variance'] = dataset.loc[:,'distance'].rolling(10,center=True).var()\n",
    "    rolling_window_summary.loc[:,'ten_record_difference'] = dataset.loc[:,'distance'].rolling(10,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'ten_record_fractional_change'] = dataset.loc[:,'distance'].rolling(10,center=True).mean().pct_change()\n",
    "   \n",
    "    return rolling_window_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54295c-a640-4ce7-a212-5974545101eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def differences_separate(dataset, rolling_window):\n",
    "\n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure_one, axis_one = plt.subplots(dates.size)\n",
    "    figure_two, axis_two = plt.subplots(dates.size)\n",
    "    figure_three, axis_three = plt.subplots(dates.size)\n",
    "    figure_four, axis_four = plt.subplots(dates.size)\n",
    "\n",
    "    figure_one.suptitle(\"Differences per record\")\n",
    "    figure_two.suptitle(\"Differences per every three records\")\n",
    "    figure_three.suptitle(\"Differences per every five records\")\n",
    "    figure_four.suptitle(\"Differences per every ten records\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis_one[i].set_xlim(min,max)\n",
    "        axis_two[i].set_xlim(min,max)\n",
    "        axis_three[i].set_xlim(min,max)\n",
    "        axis_four[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis_one[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_one[i].plot(rolling_window.index,rolling_window.loc[:,'difference'])\n",
    "        axis_two[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_two[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_difference'])\n",
    "        axis_three[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_three[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_difference'])\n",
    "        axis_four[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_four[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_difference']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'difference'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3cde2-1a51-4b85-b23c-9574015cd930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fractional_changes_separate(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure_one, axis_one = plt.subplots(dates.size)\n",
    "    figure_two, axis_two = plt.subplots(dates.size)\n",
    "    figure_three, axis_three = plt.subplots(dates.size)\n",
    "    figure_four, axis_four = plt.subplots(dates.size)\n",
    "\n",
    "    figure_one.suptitle(\"Fractional changes per record\")\n",
    "    figure_two.suptitle(\"Fractional changes per every three records\")\n",
    "    figure_three.suptitle(\"Fractional changes per every five records\")\n",
    "    figure_four.suptitle(\"Fractional changes per every ten records\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis_one[i].set_xlim(min,max)\n",
    "        axis_two[i].set_xlim(min,max)\n",
    "        axis_three[i].set_xlim(min,max)\n",
    "        axis_four[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis_one[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_one[i].plot(rolling_window.index,rolling_window.loc[:,'fractional_change'])\n",
    "        axis_two[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_two[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_fractional_change'])\n",
    "        axis_three[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_three[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_fractional_change'])\n",
    "        axis_four[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_four[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_fractional_change']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'fractional_change'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bc807-33f4-4582-bac2-db828ce009ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def means_separate(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure_one, axis_one = plt.subplots(dates.size)\n",
    "    figure_two, axis_two = plt.subplots(dates.size)\n",
    "    figure_three, axis_three = plt.subplots(dates.size)\n",
    "\n",
    "    figure_one.suptitle(\"Mean per every three records\")\n",
    "    figure_two.suptitle(\"Mean per every five records\")\n",
    "    figure_three.suptitle(\"Mean per every ten records\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis_one[i].set_xlim(min,max)\n",
    "        axis_two[i].set_xlim(min,max)\n",
    "        axis_three[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "\n",
    "        axis_one[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_one[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_mean'])\n",
    "        axis_two[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_two[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_mean'])\n",
    "        axis_three[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_three[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_mean']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_mean'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c43964-d542-43c8-81c0-581cf108f64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_deviations_separate(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure_one, axis_one = plt.subplots(dates.size)\n",
    "    figure_two, axis_two = plt.subplots(dates.size)\n",
    "    figure_three, axis_three = plt.subplots(dates.size)\n",
    "\n",
    "    figure_one.suptitle(\"Standard Deviation per every three records\")\n",
    "    figure_two.suptitle(\"Standard Deviation per every five records\")\n",
    "    figure_three.suptitle(\"Standard Deviation per every ten records\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis_one[i].set_xlim(min,max)\n",
    "        axis_two[i].set_xlim(min,max)\n",
    "        axis_three[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "\n",
    "        axis_one[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_one[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_standard_deviation'])\n",
    "        axis_two[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_two[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_standard_deviation'])\n",
    "        axis_three[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_three[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_standard_deviation']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_standard_deviation'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fcdb4-fcf6-4af1-87b7-76942dba5f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variances_separate(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure_one, axis_one = plt.subplots(dates.size)\n",
    "    figure_two, axis_two = plt.subplots(dates.size)\n",
    "    figure_three, axis_three = plt.subplots(dates.size)\n",
    "\n",
    "    figure_one.suptitle(\"Variance per every three records\")\n",
    "    figure_two.suptitle(\"Variance per every five records\")\n",
    "    figure_three.suptitle(\"Variance per every ten records\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis_one[i].set_xlim(min,max)\n",
    "        axis_two[i].set_xlim(min,max)\n",
    "        axis_three[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "\n",
    "        axis_one[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_one[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_variance'])\n",
    "        axis_two[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_two[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_variance'])\n",
    "        axis_three[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis_three[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_variance']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_variance'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d44427-40dc-49dc-b98a-05d015e3d05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def differences_combined(dataset, rolling_window):\n",
    "\n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "    figure.suptitle(\"Differences against each other\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis[i].plot(rolling_window.index,rolling_window.loc[:,'difference'])\n",
    "        axis[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_difference'])\n",
    "        axis[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_difference'])\n",
    "        axis[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_difference']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'difference'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1\n",
    "    figure.legend([\"distance\",\"difference\",\"three_record_difference\",\"five_record_difference\",\"ten_record_difference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10247c9-87eb-4e80-a090-54c58ff233f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fractional_changes_combined(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "    figure.suptitle(\"Fractional changes against each other\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis[i].plot(rolling_window.index,rolling_window.loc[:,'fractional_change'])\n",
    "        axis[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_fractional_change'])\n",
    "        axis[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_fractional_change'])\n",
    "        axis[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_fractional_change']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'fractional_change'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1\n",
    "    figure.legend([\"distance\",\"fractional_change\",\"three_record_fractional_change\",\"five_record_fractional_change\",\"ten_record_fractional_change\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824fec1-b562-4efd-949c-4213d14befbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def means_combined(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "    figure.suptitle(\"Means against each other\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "\n",
    "        axis[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_mean'])\n",
    "        axis[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_mean'])\n",
    "        axis[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_mean']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_mean'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1\n",
    "    figure.legend([\"distance\",\"three_record_mean\",\"five_record_mean\",\"ten_record_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a371b-4390-4b4b-b172-bdbe20b131f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_deviations_combined(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "    figure.suptitle(\"Standard deviations against each other\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_standard_deviation'])\n",
    "        axis[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_standard_deviation'])\n",
    "        axis[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_standard_deviation']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_standard_deviation'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1\n",
    "    figure.legend([\"distance\",\"three_record_standard_deviation\",\"five_record_standard_deviation\",\"ten_record_standard_deviation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f09c0-6bba-45d4-a10e-93080ee4ff93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variances_combined(dataset, rolling_window):\n",
    "    \n",
    "    dates = dataset.loc[:,'sample_date'].unique()\n",
    "\n",
    "    figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "    figure.suptitle(\"Variances against each other\")\n",
    "\n",
    "    #Expected number of measurements plotted\n",
    "    numOfDistances = 0;\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "        max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "        condition = dataset.sample_date==date\n",
    "\n",
    "        axis[i].set_xlim(min,max)\n",
    "\n",
    "        print(dates)\n",
    "        axis[i].plot(dataset.index,dataset.loc[:,'distance'])\n",
    "        axis[i].plot(rolling_window.loc[:,'three_record_midpoint'],rolling_window.loc[:,'three_record_variance'])\n",
    "        axis[i].plot(rolling_window.loc[:,'five_record_midpoint'],rolling_window.loc[:,'five_record_variance'])\n",
    "        axis[i].plot(rolling_window.loc[:,'ten_record_midpoint'],rolling_window.loc[:,'ten_record_variance']) \n",
    "        #Each subplot adds to total number of values plotted\n",
    "        numOfDistances = numOfDistances + rolling_window.loc[condition,'three_record_variance'].count()\n",
    "        #Checks to see if all values are plotted.\n",
    "        print(numOfDistances)\n",
    "        i+=1\n",
    "    figure.legend([\"distance\",\"three_record_variance\",\"five_record_variance\",\"ten_record_variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f4fa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea23b55-a68f-47cd-87f2-54428de1f490",
   "metadata": {},
   "source": [
    "### First Dataset Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30322c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DateTime indexes\n",
    "DH_plus_dist = DH_plus_dist.set_index(pd.to_datetime(DH_plus_dist['sample_date']+\" \"+DH_plus_dist['sample_time'], dayfirst=True), drop = False)\n",
    "# Makes sure records are in chronological order\n",
    "DH_plus_dist = DH_plus_dist.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8c02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DH_plus_dist)\n",
    "print(DH_plus_dist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7294c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",DH_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6a427-b970-4d55-87fd-15bec0eecd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8545e-2b76-4423-baf2-511fcc1670a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist = remove_duplicate_datetimes(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801c0ef-6dad-44f8-80dd-14d7ef0be54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Non function equivalent\n",
    "\n",
    "# x = DH_plus_dist.groupby(DH_plus_dist.index,sort=False)['distance'].transform('mean')\n",
    "# DH_plus_dist.loc[:,'distance'] = x.loc[:]\n",
    "# DH_plus_dist = DH_plus_dist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3740c-07e0-4223-be1f-81dfea6e432b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12913692-06a4-4ebf-90e4-170dccd032c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist, DH_hourly = hourly_summary(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422babfe-82f1-4cc2-b95d-600b94e6c5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DH_plus_dist)\n",
    "print(DH_plus_dist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0d209-dc67-4963-a96c-495c07f56dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",DH_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771c82f-94da-4b10-9503-033f1a30cd40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DH_hourly)\n",
    "print(DH_hourly.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9543f2c-1413-40c5-9c44-6432ec990106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing hourly values:\\n\",DH_hourly.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d553f-dfd7-42db-93d8-037bbd442ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=DH_plus_dist.loc[:,'distance'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f562ec-13f0-4c0b-8b37-b2e22d211a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=DH_hourly.loc[:,'distance']['mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fd962-610c-4bd5-8aaa-2861f77c091b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=DH_hourly.loc[:,'distance']['std'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bdc9c-abe2-421a-9dbc-e14798a22ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=DH_plus_dist.loc[:,'rolling_hourly_mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef698f4-b8b4-404b-a1a5-241c934be948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=DH_plus_dist.loc[:,'rolling_hourly_standard_deviation'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ea797-882c-4271-aa10-9407d5fe0bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=(DH_plus_dist.loc[:,'distance'],DH_plus_dist.loc[:,'rolling_hourly_mean']), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3722f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(DH_plus_dist,height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262231dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=DH_plus_dist[\"sample_date\"], y=DH_plus_dist[\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d277ae",
   "metadata": {},
   "source": [
    "NOTE: If I wanted to plot only time on the x-axis from the Timestamp datatype: the time attribute was returning the time object, what I needed to do was call the time() method to return the time value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda8609",
   "metadata": {},
   "source": [
    "Without Y axis range from 0 to largest distance value. Range from smallest to largest recorded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea4025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_line(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67105dbf-f330-4b11-8484-86984628df8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_plot(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d5ce2",
   "metadata": {},
   "source": [
    "With Y axis range from 0 to largest distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b1a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_line(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b5372-eb30-4843-a4d5-db8fbd39d994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_plot(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06675fc2-3af1-45d1-8e37-1c4f459f9a05",
   "metadata": {},
   "source": [
    "### First Dataset Rolling Window Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789ba91-e62b-458d-96e7-d44ad82ec954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of dataset.\n",
    "\n",
    "DH_rolling_window_summary = statistical_summary(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5715d-3d3d-418e-9bf3-68fdb2c96462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_rolling_window_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdb520-3df3-4b17-bd92-9a8b2aea7e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc50965-219b-4f19-99e0-872ac97bd3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following DH_plus_dist windowing methods.\n",
    "DH_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469773b7-16af-4736-b490-e3b4efd49ef3",
   "metadata": {},
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a3f9d-195a-403b-ad21-27c8f09666dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd851a3-60d3-4373-aaf3-1d887530a57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866da24b-2457-43c2-b34f-19ab6856bc41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a223312-bee2-4983-9ce2-5b29ef3a9834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340677c-96d6-4706-82c6-42a54bbb3b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbac2f8-a025-4718-bc8b-a30075baf032",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd14165-2562-4526-aafb-53cae40d09e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18245f83-42a1-4d89-9cbf-beba022cdddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e963852-ba4c-4e2f-a3fe-c57be76787e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913ee71-f416-41c0-a75e-184cc9687cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688071b2-4222-4c86-b4ed-6ad91401266d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc76d4",
   "metadata": {},
   "source": [
    "## Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20effe45-cbe7-486c-94e9-be4fa3ec959f",
   "metadata": {},
   "source": [
    "### Second Dataset Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea1474-aac1-4a34-ae28-a127930d2e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DateTime indexes\n",
    "LA_plus_dist = LA_plus_dist.set_index(pd.to_datetime(LA_plus_dist['sample_date']+\" \"+LA_plus_dist['sample_time'], dayfirst=True), drop = False)\n",
    "# Makes sure records are in chronological order\n",
    "LA_plus_dist = LA_plus_dist.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150997a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(LA_plus_dist)\n",
    "LA_plus_dist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2bc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",LA_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0ebd0-656c-453a-84d6-8c72885117fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be8ecd-5d15-49e1-9246-d60330adbc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_plus_dist = remove_duplicate_datetimes(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d46a18-ad0e-452c-8cd8-7325653f10b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Non function equivalent\n",
    "\n",
    "# x = LA_plus_dist.groupby(LA_plus_dist.index,sort=False)['distance'].transform('mean')\n",
    "# LA_plus_dist.loc[:,'distance'] = x.loc[:]\n",
    "# LA_plus_dist = LA_plus_dist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6480bd7-4872-4868-94c3-8e54111d1ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb542cfa-f48c-4750-a738-1ba802f656cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_plus_dist, LA_hourly = hourly_summary(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b33c4-28d3-4fad-8ef3-07b0ab9d71df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(LA_plus_dist)\n",
    "print(LA_plus_dist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810abe1-0bc0-4e00-ae97-5cfbf8d3f458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",LA_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf5ee3-3509-4a56-a22a-d405018a7fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(LA_hourly)\n",
    "print(LA_hourly.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269c1c4-1460-49c1-95f2-b3cb2449d9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing hourly values:\\n\",LA_hourly.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e924c2-8316-4e0c-98b3-adacf3a392c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_plus_dist.loc[:,'distance'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1780c5-1774-4ff3-bafe-85363a7a0cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_hourly.loc[:,'distance']['mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c43801-df7e-4c95-8287-79f036657800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_hourly.loc[:,'distance']['std'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b70e2-adeb-4d9c-a773-8cecd2721829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_plus_dist.loc[:,'rolling_hourly_mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb321c-a8d3-4c88-a2f7-2945ece7c26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_plus_dist.loc[:,'rolling_hourly_standard_deviation'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e2af0-1f9e-4ae8-b965-d7c2ad2853b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=(LA_plus_dist.loc[:,'distance'],LA_plus_dist.loc[:,'rolling_hourly_mean']), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea293a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(LA_plus_dist,height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838778b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=LA_plus_dist[\"sample_date\"], y=LA_plus_dist[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d604e7-cbd1-4973-9df1-c78c2a9118ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "something = LA_plus_dist.loc['2023-04-05':'2023-04-08']\n",
    "\n",
    "\n",
    "plt.plot(something.index, something[\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f98424",
   "metadata": {},
   "source": [
    "Without Y axis range from 0 to largest distance value. Range from smallest to largest recorded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d350441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_line(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df11cc7-b29d-4a67-9dc4-b451a8c583d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_plot(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d02600",
   "metadata": {},
   "source": [
    "With Y axis range from 0 to largest distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5deec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_line(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfef8e-08cf-49f5-bb9f-0599708f9daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_plot(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54390ca6-b91d-42c9-aeed-d905e7aed384",
   "metadata": {},
   "source": [
    "### Second Dataset Rolling Window Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dc8ca-6bfa-410e-b89b-c59f170a7edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of dataset.\n",
    "\n",
    "LA_rolling_window_summary = statistical_summary(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16db1c-1918-4f01-ba48-c0eb16d3d700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81be3dc-4043-45df-8ec9-63d9203617aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following LA_plus_dist windowing methods.\n",
    "LA_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3b94b-a233-442e-9633-09a57374d471",
   "metadata": {},
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46adfe-269e-4206-8e5c-3f49c7ac75a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab0d16-f65e-4925-a32e-4e1a4f1464e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dcc3d-b417-42a8-9c49-c313bc6bd018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20601250-35a0-4606-b995-3793b418c31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fbbd8-aef9-4682-80ad-f4442522fcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98b29f-2bb2-424e-ad8e-642f6acde1ab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf100ce-5d89-410b-97f8-6c6f19c70ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2809e49-fb8e-4e6b-b929-b762c0f05130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0efbf5-3557-4e18-9e04-947224830f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ee6c0-c6e1-4e5a-a0d5-93c88c091389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8803fef-77e6-4c8d-8492-1b9229363551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e17427",
   "metadata": {},
   "source": [
    "## Third Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f80740-8645-4190-943e-1409a0707268",
   "metadata": {},
   "source": [
    "### Third Dataset Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3aa3ad-d441-4a36-a48f-6dcb22b629e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DateTime indexes\n",
    "Lucy_16_20_May_plus_dist = Lucy_16_20_May_plus_dist.set_index(pd.to_datetime(Lucy_16_20_May_plus_dist['sample_date']+\" \"+Lucy_16_20_May_plus_dist['sample_time'], dayfirst=True), drop = False)\n",
    "# Makes sure records are in chronological order\n",
    "Lucy_16_20_May_plus_dist = Lucy_16_20_May_plus_dist.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f7f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Lucy_16_20_May_plus_dist)\n",
    "Lucy_16_20_May_plus_dist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a9310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",Lucy_16_20_May_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679fbdc-01d3-471e-8981-503bb74412b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_16_20_May_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21b874-fe20-4b7a-a7c7-b9ddd8578870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_16_20_May_plus_dist = remove_duplicate_datetimes(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c37df-2818-4ee8-a897-18bff773022b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Non function equivalent\n",
    "\n",
    "# x = Lucy_16_20_May_plus_dist.groupby(Lucy_16_20_May_plus_dist.index,sort=False)['distance'].transform('mean')\n",
    "# Lucy_16_20_May_plus_dist.loc[:,'distance'] = x.loc[:]\n",
    "# Lucy_16_20_May_plus_dist = Lucy_16_20_May_plus_dist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6c147-be3e-48df-ac0c-9ea59a18196b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_16_20_May_plus_dist.index.has_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b687393-354f-47ae-a468-13ac37ec8dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_16_20_May_plus_dist,Lucy_16_20_May_hourly = hourly_summary(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077ba5b-c50d-4cb4-b2f7-52ee06cc0c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Lucy_16_20_May_plus_dist)\n",
    "print(Lucy_16_20_May_plus_dist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a90235-4aec-4187-aedf-ccbd1bf1be7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values:\\n\",Lucy_16_20_May_plus_dist.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca3b27-6fc0-4a27-9f9a-7996a7d646b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Lucy_16_20_May_hourly)\n",
    "print(Lucy_16_20_May_hourly.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b40179-4fe6-4544-96ae-8812076149af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing hourly values:\\n\",Lucy_16_20_May_hourly.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691e18b-b583-4930-855e-e13aba7ffc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=Lucy_16_20_May_plus_dist.loc[:,'distance'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346ae00-5d3e-4713-904f-10a006411c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=Lucy_16_20_May_hourly.loc[:,'distance']['mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd82b57-0cb1-4b7b-a83e-3fe70632e09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=Lucy_16_20_May_hourly.loc[:,'distance']['std'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfd0bc-2d2b-46e2-bd24-84feda0e81d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=Lucy_16_20_May_plus_dist.loc[:,'rolling_hourly_mean'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065fd62-2f6d-4441-90e5-eede0ea3f707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=Lucy_16_20_May_plus_dist.loc[:,'rolling_hourly_standard_deviation'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1bece-be65-4d1e-a100-5b1f28f79a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=(Lucy_16_20_May_plus_dist.loc[:,'distance'],Lucy_16_20_May_plus_dist.loc[:,'rolling_hourly_mean']), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b174f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(Lucy_16_20_May_plus_dist,height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f02db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=Lucy_16_20_May_plus_dist[\"sample_date\"], y=Lucy_16_20_May_plus_dist[\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f265eb",
   "metadata": {},
   "source": [
    "Without Y axis range from 0 to largest distance value. Range from smallest to largest recorded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f5183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_line(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f34a81-d72d-4428-846f-5ed35d44b9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_range_time_series_plot(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45430cb",
   "metadata": {},
   "source": [
    "With Y axis range from 0 to largest distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890c7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_line(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eac73d-3b4e-4381-a80c-b845876ad33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absolute_range_time_series_plot(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d7753-765e-4ec3-b3ed-3f16248cb2ca",
   "metadata": {},
   "source": [
    "### Third Dataset Rolling Window Summaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e01e05-e935-4f60-85ab-cb52104a20c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of dataset.\n",
    "\n",
    "Lucy_rolling_window_summary = statistical_summary(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f197f3-5ae1-4b9e-8a47-cceb91d8e43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_rolling_window_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f02479-b97e-41d2-9984-365624bcff70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c25f5-ae01-4568-92be-68867e93881f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following DH_plus_dist windowing methods.\n",
    "Lucy_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3d939-911a-4bfd-881d-58b01b6243b4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c8c94-dc04-4a75-85c6-7e4e5211aaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b8d14-a14c-4f1f-9dc3-3811c33422d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e79ccb-7a12-4ecd-8498-555d4e924360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4006bdc-116d-4923-aa60-f6520d2a56b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fb107-e396-4dc2-bda3-571967004bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8f35c-9fac-4ac0-9c4e-2fa4b4384a17",
   "metadata": {},
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c919d3-8280-420c-acce-6a430edda87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03a150-c925-4aae-a3f9-92cc43628e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a77f28-2cae-4e5e-96e4-bb52681a4417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177b045-7e22-400a-a6f4-fb9a259a6758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86aff9-9e93-4555-9d3b-c5c008789ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548b9d3-3caa-4675-9e48-594d5764d339",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072223b-cf36-4a7d-b45f-e3d52d40b2ff",
   "metadata": {},
   "source": [
    "1. Load datasets.\n",
    "2. Format and add datetime indexes to each record\n",
    "3. Apply custom method for removing duplicate records by using the mean distance value of mulitple distance values\n",
    "4. Remove records where the distancer value is 0. We know that the bracelet has been taken off and shouldn't be picking up new observations.\n",
    "\n",
    "n. Final step display summary of the processed data, to compare raw data against new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fb33b-18ae-4608-955a-77c00aba8a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_hour_mean = dataset.loc[:,'distance'].rolling('1h',center=True).mean()\n",
    "    rolling_hour_standard_deviation = dataset.loc[:,'distance'].rolling('1h',center=True).std()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hour_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hour_standard_deviation\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    #sns.displot(data=dataset.loc[:,'distance'], kde=True)\n",
    "    #sns.boxplot(x=dataset[\"sample_date\"], y=dataset[\"distance\"])\n",
    "    #plt.plot(dataset.loc[:,'distance'])\n",
    "    #plt.plot(dataset.loc[:,'distance'].rolling('1h',center=True).std(),'x')\n",
    "    relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #absolute_range_time_series_line(dataset.loc[:,'distance'].rolling('1h',center=True).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be00b28-882a-46bb-b6a1-e1e9cb9e89c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5429c2-a49e-4d57-bb45-ed0dd921c0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e17f5-7d45-459d-b801-66efe14824a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668b29f-a514-457e-bbb6-091a63cec957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_rolling_window_imputation(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_hour_mean = dataset.loc[:,'distance'].rolling('1h',center=True).mean()\n",
    "    rolling_hour_standard_deviation = dataset.loc[:,'distance'].rolling('1h',center=True).std()\n",
    "    rolling_ten_record_mean = dataset.loc[:,'distance'].rolling(10,center=True).mean()\n",
    "    rolling_ten_record_standard_deviation = dataset.loc[:,'distance'].rolling(10,center=True).std()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hour_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hour_standard_deviation\n",
    "    dataset['rolling_ten_record_mean'] = rolling_ten_record_mean\n",
    "    dataset['rolling_ten_record_standard_deviation'] = rolling_ten_record_standard_deviation\n",
    "    \n",
    "    # Removes too many records and there are still distinct outliers present\n",
    "    # dataset = dataset.loc[dataset.distance < (dataset.rolling_hourly_mean-dataset.rolling_hourly_standard_deviation).abs(),:]\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"distance\"] + dataset[\"distance\"].std())) & (dataset[\"distance\"] > (dataset[\"distance\"] - dataset[\"distance\"].std()))\n",
    "    #(dataframe1['column'] == \"expression\") & (dataframe1['column'] != \"another expression)\n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "\n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "\n",
    "    relative_range_time_series_plot(dataset)\n",
    "    print(dataset.loc[dataset.satisfied_filtering_condition == True,'satisfied_filtering_condition'].shape)\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128efaba-2874-4537-8c09-c160668a536e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652682c-bd56-4bde-96b0-e80454275297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6e689-dab4-406b-8bf0-d683479ad423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf95b4-03f0-4fc0-97ba-90b068aa737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_rolling_window_imputation_two(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_ten_minutes_mean = dataset.loc[:,'distance'].rolling('10min',center=True).mean()\n",
    "    rolling_ten_minutes_standard_deviation = dataset.loc[:,'distance'].rolling('10min',center=True).std()\n",
    "    dataset['rolling_ten_minutes_mean'] = rolling_ten_minutes_mean\n",
    "    dataset['rolling_ten_minutes_standard_deviation'] = rolling_ten_minutes_standard_deviation\n",
    "    \n",
    "    # Removes too many records and there are still distinct outliers present\n",
    "    # dataset = dataset.loc[dataset.distance < (dataset.rolling_hourly_mean-dataset.rolling_hourly_standard_deviation).abs(),:]\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"rolling_ten_minutes_mean\"] < (dataset[\"distance\"] + dataset[\"rolling_ten_minutes_standard_deviation\"])) & (dataset[\"rolling_ten_minutes_mean\"] > (dataset[\"distance\"] - dataset[\"rolling_ten_minutes_standard_deviation\"]))\n",
    "    #(dataframe1['column'] == \"expression\") & (dataframe1['column'] != \"another expression)\n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    # Already tried this\n",
    "    #std = dataset.distance.std()\n",
    "    #print(\"Standard Deviation:\" ,std)\n",
    "    #print(dataset.loc[(dataset.distance < (dataset.distance + std)) & (dataset.distance > (dataset.distance - std)),'distance'].count())\n",
    "\n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    \n",
    "    satisfied_condition_record_count(dataset)\n",
    "    relative_range_time_series_plot(dataset,'rolling_ten_minutes_mean','rolling_ten_minutes_standard_deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbf448-dc7b-4267-b4fa-e935982ce6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_two(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f914e-9d56-4395-9259-57863067e545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_two(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea95ab3-fa1b-4dd1-8f2e-694c7dd547bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_two(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d2066-53a9-46ca-b078-1a88c099c11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_rolling_window_imputation_three(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_fifty_record_mean = dataset.loc[:,'distance'].rolling(50,center=True).mean()\n",
    "    rolling_fifty_record_standard_deviation = dataset.loc[:,'distance'].rolling(50,center=True).std()\n",
    "    dataset['rolling_fifty_record_mean'] = rolling_fifty_record_mean\n",
    "    dataset['rolling_fifty_record_standard_deviation'] = rolling_fifty_record_standard_deviation\n",
    "    \n",
    "    # Removes too many records and there are still distinct outliers present\n",
    "    # dataset = dataset.loc[dataset.distance < (dataset.rolling_hourly_mean-dataset.rolling_hourly_standard_deviation).abs(),:]\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"rolling_fifty_record_mean\"] < (dataset[\"distance\"] + dataset[\"rolling_fifty_record_standard_deviation\"])) & (dataset[\"rolling_fifty_record_mean\"] > (dataset[\"distance\"] - dataset[\"rolling_fifty_record_standard_deviation\"]))\n",
    "    #(dataframe1['column'] == \"expression\") & (dataframe1['column'] != \"another expression)\n",
    "    print(\"Total number of records meeting filtering condition:\\n\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "\n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    \n",
    "    satisfied_condition_record_count(dataset)\n",
    "    relative_range_time_series_plot(dataset,'rolling_fifty_record_mean','rolling_fifty_record_standard_deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7a9a8-fe61-47e1-a3c7-6b15cc1fd1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_three(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c153c7-bfbe-482f-8ebc-d2262656c23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_three(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a7452-34b4-4a30-9573-1f7f79b7ea79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_engineering_rolling_window_imputation_three(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a0a58-b2f1-4ff2-b65e-dd4c41d7be14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hourly_box_plots(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    dataset['hour'] = dataset.index.hour\n",
    "    sns.boxplot(x=dataset[\"hour\"], y=dataset[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61e420-9061-423b-86cc-1cdc55dbde5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_box_plots(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcf402-0671-4ed1-85c3-36469b053f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_box_plots(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ff033-9df3-4e19-81f1-2d4e9a36112f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_box_plots(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59129ca-cb54-4bed-9bd8-c2de54182b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hourly_average_box_plot(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.resample('h').mean()\n",
    "    print(dataset)\n",
    "    print(dataset.describe())\n",
    "    sns.boxplot(x=dataset['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2bb72-f466-4417-bcc7-e8b7464e7ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_average_box_plot(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcac98-4ce3-4064-838c-c87a188772a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_average_box_plot(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea260032-2869-4d30-b178-3f12c7e80bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_average_box_plot(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fcd7e-8fe6-4024-a7a5-39d3481c5a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greater_than_lower_quartile(dataset_path):\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_hour_mean = dataset.loc[:,'distance'].rolling('1h',center=True).mean()\n",
    "    rolling_hour_standard_deviation = dataset.loc[:,'distance'].rolling('1h',center=True).std()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hour_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hour_standard_deviation\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = dataset[\"distance\"] > dataset[\"distance\"].quantile(0.25)\n",
    "    #(dataframe1['column'] == \"expression\") & (dataframe1['column'] != \"another expression)\n",
    "    print(\"Total number of records above the lower quartile:\\n\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "    \n",
    "    relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ec4fd-9b8d-47e5-9dbc-31338b24e4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greater_than_lower_quartile(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0d487-8a84-4300-a8a2-5fd676704b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greater_than_lower_quartile(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a39347-8039-41f4-a21a-7bfcabfb457e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greater_than_lower_quartile(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6d08b-5e6f-40ad-98f3-8cc8f378bb96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stationary(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_hour_mean = dataset.loc[:,'distance'].rolling('1h',center=True).mean()\n",
    "    rolling_hour_standard_deviation = dataset.loc[:,'distance'].rolling('1h',center=True).std()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hour_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hour_standard_deviation\n",
    "    dataset['difference'] = dataset[\"distance\"].diff()\n",
    "    \n",
    "    \n",
    "    rolling_daily_mean = dataset.loc[:,'distance'].rolling('1D',center=True).mean()\n",
    "    rolling_daily_standard_deviation = dataset.loc[:,'distance'].rolling('1D',center=True).std()\n",
    "    dataset['rolling_daily_mean'] = rolling_daily_mean\n",
    "    dataset['rolling_daily_standard_deviation'] = rolling_daily_standard_deviation\n",
    "    \n",
    "    ten_record_difference = dataset.loc[:,'distance'].rolling(10,center=True).mean().diff()\n",
    "    dataset[\"ten_record_difference\"] = ten_record_difference\n",
    "    #rolling_ten_record_mean = dataset.loc[:,'distance'].rolling(10,center=True).mean()\n",
    "    #rolling_ten_record_standard_deviation = dataset.loc[:,'distance'].rolling(10,center=True).std()\n",
    "    #dataset['rolling_ten_record_mean'] = rolling_ten_record_mean\n",
    "    #dataset['rolling_ten_record_standard_deviation'] = rolling_ten_record_standard_deviation\n",
    "    \n",
    "    #rolling_fifty_record_mean = dataset.loc[:,'distance'].rolling(50,center=True).mean()\n",
    "    #rolling_fifty_record_standard_deviation = dataset.loc[:,'distance'].rolling(50,center=True).std()\n",
    "    #dataset['rolling_fifty_record_mean'] = rolling_fifty_record_mean\n",
    "    #dataset['rolling_fifty_record_standard_deviation'] = rolling_fifty_record_standard_deviation\n",
    "    \n",
    "    #five_record_mean = dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "    #five_record_standard_deviation = dataset.loc[:,'distance'].rolling(5,center=True).std()\n",
    "    #dataset['five_record_mean'] = five_record_mean\n",
    "    #dataset['five_record_standard_deviation'] = five_record_standard_deviation\n",
    "    \n",
    "    twenty_five_record_mean = dataset.loc[:,'distance'].rolling(25,center=True).mean()\n",
    "    twenty_five_record_standard_deviation = dataset.loc[:,'distance'].rolling(25,center=True).std()\n",
    "    dataset['twenty_five_record_mean'] = twenty_five_record_mean\n",
    "    dataset['twenty_five_record_standard_deviation'] = twenty_five_record_standard_deviation\n",
    "    twenty_five_record_difference = dataset.loc[:,'distance'].rolling(25,center=True).mean().diff()\n",
    "    dataset['twenty_five_record_difference'] = twenty_five_record_difference\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] > (dataset[\"distance\"].mean() - dataset[\"distance\"].std())) & (dataset[\"distance\"] < (dataset[\"distance\"].mean() + dataset[\"distance\"].std()))\n",
    "    #(dataframe1['column'] == \"expression\") & (dataframe1['column'] != \"another expression)\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] > (dataset[\"rolling_hourly_mean\"] - dataset[\"rolling_hourly_standard_deviation\"])) & (dataset[\"distance\"] < (dataset[\"rolling_hourly_mean\"] + dataset[\"rolling_hourly_standard_deviation\"]))\n",
    "    #print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] > (dataset[\"rolling_ten_record_mean\"] - dataset[\"rolling_ten_record_standard_deviation\"])) & (dataset[\"distance\"] < (dataset[\"rolling_ten_record_mean\"] + dataset[\"rolling_ten_record_standard_deviation\"]))\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"rolling_ten_record_mean\"] > (dataset[\"distance\"] - dataset[\"rolling_ten_record_standard_deviation\"]))\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"rolling_ten_record_standard_deviation\"]<dataset[\"rolling_ten_record_mean\"])\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"rolling_hourly_standard_deviation\"]<dataset[\"rolling_hourly_mean\"])\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"five_record_standard_deviation\"]<dataset[\"five_record_mean\"])\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff() < dataset[\"rolling_hourly_standard_deviation\"]) # Use for changing betwen passive and active staes possibly. standard deviation or variance for statiarity.\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < dataset[\"distance\"].std()**2)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < dataset[\"rolling_daily_mean\"] + dataset[\"rolling_daily_standard_deviation\"])\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"ten_record_difference\"].abs() < dataset[\"distance\"].std())\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = ((dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + dataset[\"distance\"].std())) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - dataset[\"distance\"].std())))\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"twenty_five_record_difference\"].abs()<=0.25)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = ((dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + dataset[\"twenty_five_record_standard_deviation\"])) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - dataset[\"twenty_five_record_standard_deviation\"])))\n",
    "    \n",
    "    \n",
    "    # Try out 5, 15 and 25 records and see how they compare.\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"twenty_five_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could cont the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['twenty_five_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    \n",
    "    print(dataset.groupby(dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")))\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "\n",
    "    #relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #relative_range_time_series_plot(dataset,'difference')\n",
    "    #relative_range_time_series_plot(dataset,'ten_record_difference')\n",
    "    relative_range_time_series_plot(dataset,'twenty_five_record_mean','twenty_five_record_standard_deviation','twenty_five_record_difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2df13f-aa69-4544-a0e9-08f497cf03ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stationary(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49ab36-2b69-4764-a0f7-38ce11ecdf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stationary(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d8f9a-e6d6-47a0-89da-0ec924749b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stationary(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859043f0-afdf-4920-96fb-b25cdc45d986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_term(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    \n",
    "    twenty_five_record_mean = dataset.loc[:,'distance'].rolling(25,center=True).mean()\n",
    "    twenty_five_record_standard_deviation = dataset.loc[:,'distance'].rolling(25,center=True).std()\n",
    "    dataset['twenty_five_record_mean'] = twenty_five_record_mean\n",
    "    dataset['twenty_five_record_standard_deviation'] = twenty_five_record_standard_deviation\n",
    "    twenty_five_record_difference = dataset.loc[:,'distance'].rolling(25,center=True).mean().diff()\n",
    "    dataset['twenty_five_record_difference'] = twenty_five_record_difference\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"twenty_five_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could count the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['twenty_five_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.1) # Alternative method to determine passive periods.\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 0.5) | (dataset['twenty_five_record_standard_deviation'] <= 0.25)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 1)\n",
    "    \n",
    "    subset = dataset.loc[dataset.satisfied_filtering_condition == False,:].index.to_frame(name='false_condition_interval')\n",
    "    #othersubset = subset.index.to_frame(name='false_condition_interval')\n",
    "    subset['false_condition_interval'] = subset.diff()\n",
    "    #othersubset['shifted_false_datetime'] = subset.index.to_series().diff()\n",
    "    print(subset)\n",
    "    #print(subset.index.to_series().diff())\n",
    "    #subset['shifted_false_datetime'] = pd.to_datetime(subset['sample_date']+\" \"+subset['sample_time'], dayfirst=True)\n",
    "    #subset['shifted_false_datetime'] = subset['shifted_false_datetime'].diff()\n",
    "    \n",
    "    \n",
    "    #print(\"Subset:\", subset)\n",
    "\n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    dataset = pd.concat([dataset,subset], axis=1)\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "\n",
    "    #relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #relative_range_time_series_plot(dataset,'difference')\n",
    "    #relative_range_time_series_plot(dataset,'ten_record_difference')\n",
    "    relative_range_time_series_plot(dataset,'twenty_five_record_mean','twenty_five_record_standard_deviation','twenty_five_record_difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe02d41-fd26-4bb1-a4a3-ce216e8d60a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def middle_term(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    rolling_fifteen_record_mean = dataset.loc[:,'distance'].rolling(15,center=True).mean()\n",
    "    rolling_fifteen_record_standard_deviation = dataset.loc[:,'distance'].rolling(15,center=True).std()\n",
    "    dataset['rolling_fifteen_record_mean'] = rolling_fifteen_record_mean\n",
    "    dataset['rolling_fifteen_record_standard_deviation'] = rolling_fifteen_record_standard_deviation\n",
    "    rolling_fifteen_record_difference = dataset.loc[:,'distance'].rolling(15,center=True).mean().diff()\n",
    "    dataset['rolling_fifteen_record_difference'] = rolling_fifteen_record_difference\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"rolling_fifteen_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could count the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"rolling_fifteen_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"rolling_fifteen_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['rolling_fifteen_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Alternative method to determine passive periods.\n",
    "    \n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "\n",
    "    #relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #relative_range_time_series_plot(dataset,'difference')\n",
    "    #relative_range_time_series_plot(dataset,'ten_record_difference')\n",
    "    relative_range_time_series_plot(dataset,'rolling_fifteen_record_mean','rolling_fifteen_record_standard_deviation','rolling_fifteen_record_difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba6fce-3dd8-45e4-ac21-5921dbc5245b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def short_term(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    five_record_mean = dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "    five_record_standard_deviation = dataset.loc[:,'distance'].rolling(5,center=True).std()\n",
    "    dataset['five_record_mean'] = five_record_mean\n",
    "    dataset['five_record_standard_deviation'] = five_record_standard_deviation\n",
    "    five_record_difference = dataset.loc[:,'distance'].rolling(5,center=True).mean().diff()\n",
    "    dataset['five_record_difference'] = five_record_difference\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"five_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could count the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"five_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"five_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['five_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Alternative method to determine passive periods.\n",
    "    \n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "\n",
    "    #relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #relative_range_time_series_plot(dataset,'difference')\n",
    "    #relative_range_time_series_plot(dataset,'ten_record_difference')\n",
    "    relative_range_time_series_plot(dataset,'five_record_mean','five_record_standard_deviation','five_record_difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483516-3dd8-45de-adbd-a1a753e2606c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_term(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d79b82-e727-423c-8541-c760e36310b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_term(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897f139-907d-43eb-8a4b-a80d75f718da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_term(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b850dc-872c-4b76-84f2-3c4c8c7b384d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "middle_term(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841c871-a8c9-4825-8565-793afa765aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "middle_term(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f77f8b-a045-44a2-ac07-ca70f85f7c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "middle_term(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58acb65-33b4-4b6c-9253-e2ab29c77d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_term(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7abea5-c58f-4413-b9d2-68b96b3da3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_term(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d4385-b00b-47d5-8797-78470f6a787b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_term(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa6842-232b-404a-b4df-b327b7e04ab0",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756c628-29ff-43ff-9aad-f95e02bc5cd6",
   "metadata": {},
   "source": [
    "## Final Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dd3ed-b695-4e87-a88b-fce4c7600516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_window_with_custom_window(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    #print(dataset)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    \n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    #Drop columns with missing values because datasets 1 and 2 each have one missing distance value which is missing completely at random.\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    dataset['lag'] = dataset.index\n",
    "    dataset['lag'] = dataset['lag'].diff()\n",
    "    print(\"Number of lags greater than 15 minutes:\", (dataset[dataset.lag > pd.Timedelta(15, \"m\")]).count())\n",
    "    \n",
    "    five_record_custom_indexer = CustomIndexer(dataset = dataset, dynamic_window_size = 5)\n",
    "    \n",
    "    twenty_five_record_custom_indexer = CustomIndexer(dataset = dataset, dynamic_window_size = 25)\n",
    "\n",
    "    five_record_mean = dataset.loc[:,'distance'].rolling(window=five_record_custom_indexer).mean()\n",
    "    five_record_standard_deviation = dataset.loc[:,'distance'].rolling(window=five_record_custom_indexer).std()\n",
    "    five_record_difference = dataset.loc[:,'distance'].rolling(window=five_record_custom_indexer).mean().diff()\n",
    "    dataset['five_record_mean'] = five_record_mean\n",
    "    dataset['five_record_standard_deviation'] = five_record_standard_deviation\n",
    "    dataset['five_record_difference'] = five_record_difference\n",
    "    \n",
    "    twenty_five_record_mean = dataset.loc[:,'distance'].rolling(window=twenty_five_record_custom_indexer).mean()\n",
    "    twenty_five_record_standard_deviation = dataset.loc[:,'distance'].rolling(window=twenty_five_record_custom_indexer).std()\n",
    "    twenty_five_record_difference = dataset.loc[:,'distance'].rolling(window=twenty_five_record_custom_indexer).mean().diff()\n",
    "    dataset['twenty_five_record_mean'] = twenty_five_record_mean\n",
    "    dataset['twenty_five_record_standard_deviation'] = twenty_five_record_standard_deviation\n",
    "    dataset['twenty_five_record_difference'] = twenty_five_record_difference\n",
    "    \n",
    "    rolling_fifteen_minute_mean = dataset.loc[:,'distance'].rolling('15min',center=True).mean()\n",
    "    rolling_fifteen_minute_standard_deviation = dataset.loc[:,'distance'].rolling('15min',center=True).std()\n",
    "    rolling_fifteen_minute_difference = dataset.loc[:,'distance'].rolling('15min',center=True).mean().diff()\n",
    "    dataset['rolling_fifteen_minute_mean'] = rolling_fifteen_minute_mean\n",
    "    dataset['rolling_fifteen_minute_standard_deviation'] = rolling_fifteen_minute_standard_deviation\n",
    "    dataset['rolling_fifteen_minute_difference'] = rolling_fifteen_minute_difference\n",
    "    \n",
    "    rolling_hourly_mean = dataset.loc[:,'distance'].rolling('1h', center=True).mean()\n",
    "    rolling_hourly_standard_deviation = dataset.loc[:,'distance'].rolling('1h', center=True).std()\n",
    "    rolling_hourly_difference = dataset.loc[:,'distance'].rolling('1h', center=True).mean().diff()\n",
    "    dataset['rolling_hourly_mean'] = rolling_hourly_mean\n",
    "    dataset['rolling_hourly_standard_deviation'] = rolling_hourly_standard_deviation\n",
    "    dataset['rolling_hourly_difference'] = rolling_hourly_difference\n",
    "    \n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"twenty_five_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could count the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['twenty_five_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.1) # Alternative method to determine passive periods.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 0.25) | (dataset['twenty_five_record_standard_deviation'] <= 0.25)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 1) \n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 0.5) | (dataset['twenty_five_record_standard_deviation'] <= 0.25)\n",
    "    \n",
    "    subset = dataset.loc[dataset.satisfied_filtering_condition == False,:].index.to_frame(name='false_condition_interval')\n",
    "\n",
    "    subset['false_condition_interval'] = subset.diff()\n",
    "\n",
    "    print(subset)\n",
    "\n",
    "\n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    dataset = pd.concat([dataset,subset], axis=1)\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "    satisfied_condition_record_count(dataset)\n",
    "    \n",
    "    dataset = dataset.loc[dataset[\"satisfied_filtering_condition\"] == True ]\n",
    "    \n",
    "    relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation','rolling_hourly_difference')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f98de-e560-4c2b-b915-9a1078cb527e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_one = rolling_window_with_custom_window(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03df200-b0f5-40a8-88e0-89a69104abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=dataset_one[\"sample_date\"], y=dataset_one[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab91d7-eac5-4b2b-ae9e-23915719c01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertia_elbow_method_of_whole_duration(dataset_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d93d2a-ee98-430f-bce9-127ada9cfc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_two = rolling_window_with_custom_window(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8525f8f-2714-46b1-b0bd-8ffc4b721a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=dataset_two[\"sample_date\"], y=dataset_two[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040cea2-38e5-4d6f-a533-92b880811901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertia_elbow_method_of_whole_duration(dataset_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd89830-19ec-4ab1-81f8-130f1856a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_three = rolling_window_with_custom_window(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ac1a1-f20a-4da9-a6b0-ebbb131b14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=dataset_three[\"sample_date\"], y=dataset_three[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c087-607d-42ac-a601-df1acfbda521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertia_elbow_method_of_whole_duration(dataset_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb0012-e517-4d39-96a0-76d93c02ecd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_means_clustering_of_whole_duration(dataset_one,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c8e96-5fd8-4af8-a149-0e5ca74a9197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_means_clustering_of_whole_duration(dataset_two,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad3d2b-197e-414c-8057-84806a585014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_means_clustering_of_whole_duration(dataset_three,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f66c3-4c42-4ab8-b490-198b66378d8e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Mischelanious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4845688-b530-4a0c-b75d-2e802b66017b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DH_plus_dist.loc[:,'distance'].rolling(10,center=True).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c6512-2419-4718-82a8-47406cc3068c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True).quantile(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d8ffb-0590-4b37-8014-a2f871677b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DH_plus_dist.loc[:,'distance'].index[0])\n",
    "a = (DH_plus_dist.loc[:,'distance'].index[1] - DH_plus_dist.loc[:,'distance'].index[0])/2\n",
    "b = DH_plus_dist.loc[:,'distance'].index[0] + a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98156588-e1ca-46d3-9e7f-c35cdf9a6121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True).sum().index[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28df101-253b-4cf9-a8cb-0adc33ab2191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True).sum().apply(lambda x: print(x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0361a8-bd62-4ace-82bb-55047e344100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "even = DH_plus_dist.loc[:,'distance'].index[0::2]\n",
    "odd = DH_plus_dist.loc[:,'distance'].index[1::2]\n",
    "diff = odd-even\n",
    "mid = diff/2\n",
    "print(even.size)\n",
    "print(odd)\n",
    "print(diff.size)\n",
    "print(mid)\n",
    "even+mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b041b1-9fc0-4cef-8450-2ae734fb4974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def midpoint(window_size):\n",
    "    multiples=[]\n",
    "    for i in range(window_size):\n",
    "        multiples.append(DH_plus_dist.loc[:,'distance'].index[i::window_size])\n",
    "    print(multiples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854f863-709d-494b-b3ad-7d8b9e6d982c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "midpoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d73a5-ad29-4901-836e-3ca7eb9fbc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(DH_plus_dist.loc[:,'distance'].rolling(3,center=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cd700-f3a3-43d1-9869-f4219554feb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir('pandas.core.window.rolling.Rolling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a32a8-0511-4dc0-b3fa-f44f8152a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True).__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165c77-5c1d-4d24-ba35-7c3191287f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "#DH_rolling_window_summary.loc[:,'difference'] = DH_plus_dist.loc[:,'distance'].diff()\n",
    "temp.loc[:,'test'] = DH_plus_dist.loc[:,'distance'].rolling(3,center=True,min_periods=3).mean()\n",
    "idx = DH_plus_dist.loc[:,'distance'].index\n",
    "print(len(idx))\n",
    "all = []\n",
    "for i in range(idx.size-1):\n",
    "    #print(idx[i])\n",
    "    earliest = idx[i-1]\n",
    "    latest = idx[i+1]\n",
    "    #halfdiff = (latest - earliest) / 2\n",
    "    #final = earliest+halfdiff\n",
    "    \n",
    "    midpoint = pd.Interval(earliest,latest,closed='both').mid\n",
    "    all.append(midpoint)\n",
    "    #all.append(final)\n",
    "    #print(idx[i-1],idx[i],idx[i+1])\n",
    "    #print((idx[i+1]-idx[i-1])/2)\n",
    "    #print()\n",
    "print(len(all))\n",
    "print(temp.count())\n",
    "print(temp.shape)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4567be1-7127-4380-81ed-c373181b7e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame(index=DH_plus_dist.index)\n",
    "# The RolingWindow object automatically checks for if the Window size exceeds the min number of records per window by throwing an exception.\n",
    "#temp.loc[:,'test'] = DH_plus_dist.loc[:,'distance'].rolling(3,center=True,min_periods=3).mean()\n",
    "\n",
    "# Because midpoint index is always rounded up if the window size is even\n",
    "minShift = round(3/2)\n",
    "maxShift = int(3/2)\n",
    "indexes = DH_plus_dist.index # or indexes = temp.index\n",
    "newIndexes = []\n",
    "count = 0\n",
    "\n",
    "# This is to check the positon of the indexes\n",
    "for i in range(indexes.size) :\n",
    "\n",
    "    #earliest = indexes[i-minShift]\n",
    "    #latest = indexes[i+maxShift]\n",
    "    \n",
    "    # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "    # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "    if i - minShift < 0:\n",
    "        earliest = indexes[0]\n",
    "        latest = indexes[i+maxShift]\n",
    "    elif i + maxShift > indexes.size-1:\n",
    "        earliest = indexes[i-minShift]\n",
    "        latest = indexes[indexes.size-1]\n",
    "    else:\n",
    "        earliest = indexes[i-minShift]\n",
    "        latest = indexes[i+maxShift]\n",
    "\n",
    "    midpoint = pd.Interval(earliest, latest, closed='both').mid\n",
    "    newIndexes.append(midpoint)\n",
    "    #count+=1\n",
    "    #print(i)\n",
    "\n",
    "#temp.reset_index(drop=True)\n",
    "#temp.set_index(newIndexes)\n",
    "print(len(DH_plus_dist))\n",
    "print(len(newIndexes))\n",
    "print(temp.index.size)\n",
    "#print(count)\n",
    "#print(DH_plus_dist.loc[:,'distance'])\n",
    "val = DH_plus_dist.loc[:,'distance']\n",
    "val.reset_index(drop=True)\n",
    "print(val)\n",
    "\n",
    "temp2 = pd.DataFrame(data = {'col1':newIndexes,'col2':val})\n",
    "#temp.loc[:'val'] = DH_plus_dist.loc[:,'distance']\n",
    "temp2.reset_index(drop=True, inplace=True)\n",
    "temp2.set_index('col1')\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af11e69-8d8f-4188-97a1-8f3d147d22dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp2.set_index('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd93736-5aab-4c1b-9767-58018e71322f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(temp2.col1,temp2.col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca65b81-f2c6-4b36-8cfc-7a2660c7e997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def midpoints(window_size,dataframe):\n",
    "    \n",
    "    # Because midpoint index is always rounded up if the window size is even\n",
    "    minShift = round(window_size/2)\n",
    "    maxShift = int(window_size/2)\n",
    "    indexes = dataframe.index # or indexes = temp.index\n",
    "    newIndexes = []\n",
    "    count = 0\n",
    "\n",
    "    # This is to check the positon of the indexes\n",
    "    for i in range(indexes.size) :\n",
    "\n",
    "        #earliest = indexes[i-minShift]\n",
    "        #latest = indexes[i+maxShift]\n",
    "\n",
    "        # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "        # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "        if i - minShift < 0:\n",
    "            earliest = indexes[0]\n",
    "            latest = indexes[i+maxShift]\n",
    "        elif i + maxShift > indexes.size-1:\n",
    "            earliest = indexes[i-minShift]\n",
    "            latest = indexes[indexes.size-1]\n",
    "        else:\n",
    "            earliest = indexes[i-minShift]\n",
    "            latest = indexes[i+maxShift]\n",
    "\n",
    "        midpoint = pd.Interval(earliest, latest, closed='both').mid\n",
    "        newIndexes.append(midpoint)\n",
    "    \n",
    "    return newIndexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d342e56-bd8f-48cc-81aa-b14886b1301d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistical_summary(test_dataset):\n",
    "    #temp = pd.DataFrame(index=test_dataset.index)\n",
    "    # The RolingWindow object automatically checks for if the Window size exceeds the min number of records per window by throwing an exception.\n",
    "    #temp.loc[:,'test'] = test_dataset.loc[:,'distance'].rolling(3,center=True,min_periods=3).mean()\n",
    "\n",
    "    #Can also do it this way.\n",
    "    #DH_minute_rolling_window_summary = pd.DataFrame(index=test_dataset.index, data = {'mean':test_dataset.loc[:,'distance'].rolling('min').mean(),'std':test_dataset.loc[:,'distance'].rolling('min').std(),'var':test_dataset.loc[:,'distance'].rolling('min').var()})\n",
    "    rolling_window_summary = pd.DataFrame()\n",
    "    rolling_window_summary.loc[:,'difference'] = test_dataset.loc[:,'distance'].diff()\n",
    "    rolling_window_summary.loc[:,'fractional_change'] = test_dataset.loc[:,'distance'].pct_change()\n",
    "    \n",
    "    rolling_window_summary.loc[:,'three_record_midpoint'] = midpoints(3, test_dataset)\n",
    "    rolling_window_summary.loc[:,'three_record_mean'] = test_dataset.loc[:,'distance'].rolling(3,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'three_record_standard_deviation'] = test_dataset.loc[:,'distance'].rolling(3,center=True).std()\n",
    "    rolling_window_summary.loc[:,'three_record_variance'] = test_dataset.loc[:,'distance'].rolling(3,center=True).var()\n",
    "    rolling_window_summary.loc[:,'three_record_difference'] = test_dataset.loc[:,'distance'].rolling(3,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'three_record_fractional_change'] = test_dataset.loc[:,'distance'].rolling(3,center=True).mean().pct_change()\n",
    "\n",
    "    rolling_window_summary.loc[:,'five_record_midpoint'] = midpoints(5, test_dataset)\n",
    "    rolling_window_summary.loc[:,'five_record_mean'] = test_dataset.loc[:,'distance'].rolling(5,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'five_record_standard_deviation'] = test_dataset.loc[:,'distance'].rolling(5,center=True).std()\n",
    "    rolling_window_summary.loc[:,'five_record_variance'] = test_dataset.loc[:,'distance'].rolling(5,center=True).var()\n",
    "    rolling_window_summary.loc[:,'five_record_difference'] = test_dataset.loc[:,'distance'].rolling(5,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'five_record_fractional_change'] = test_dataset.loc[:,'distance'].rolling(5,center=True).mean().pct_change()\n",
    "\n",
    "    rolling_window_summary.loc[:,'ten_record_midpoint'] = midpoints(10, test_dataset)\n",
    "    rolling_window_summary.loc[:,'ten_record_mean'] = test_dataset.loc[:,'distance'].rolling(10,center=True).mean()\n",
    "    rolling_window_summary.loc[:,'ten_record_standard_deviation'] = test_dataset.loc[:,'distance'].rolling(10,center=True).std()\n",
    "    rolling_window_summary.loc[:,'ten_record_variance'] = test_dataset.loc[:,'distance'].rolling(10,center=True).var()\n",
    "    rolling_window_summary.loc[:,'ten_record_difference'] = test_dataset.loc[:,'distance'].rolling(10,center=True).mean().diff()\n",
    "    rolling_window_summary.loc[:,'ten_record_fractional_change'] = test_dataset.loc[:,'distance'].rolling(10,center=True).mean().pct_change()\n",
    "   \n",
    "    return rolling_window_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d9326-cec1-467a-85a8-7633c339c349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistical_summary(LA_plus_dist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86284225-eef1-4ef0-97e4-d9fed7b1a06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistical_summary(LA_plus_dist).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c995148-75a0-4620-8d9a-6eb21e685aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be4838-b06a-43c9-a078-7663a8ba835a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int(135.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f98151-f163-4068-83ba-04c94e02b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_2017 = pd.Interval(pd.Timestamp('2017-01-01 00:00:00'),\n",
    "                        pd.Timestamp('2017-01-01 00:00:00'),\n",
    "                        closed='both')\n",
    "year_2017.mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e9d31-f513-4c20-a22c-8fff6eaf9b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8f215-7569-4dae-8890-b623b00e58de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505de24b-60ab-4539-a603-9933bda6b741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True,min_periods=3,closed='both').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0ab5b-cfdc-4555-87d2-2d0818da70b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True,min_periods=3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4d2b2-7086-4d22-a27c-87c287ee2c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3,center=True,min_periods=3, closed='left').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37434a26-ccf4-4ada-9691-6f242ee8fd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece85a6c-17fe-4dd9-a6c2-137cf5377351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_plus_dist.loc[:,'distance'].rolling(1,center=True,closed='both').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7ee46-2d13-4dab-a303-4799d876310a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = pd.Series([2, 3, 7, 9, 19, 12, 14])\n",
    "print(s.rolling(4,center=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904627b5-ed8f-488d-bd45-673b8bb90f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_rolling_window_summary = statistical_summary(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905638b0-f99e-49e4-b283-6c1d77f4aade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = DH_plus_dist.loc[:,'sample_date'].unique()\n",
    "\n",
    "figure, axis = plt.subplots(dates.size)\n",
    "\n",
    "figure.suptitle(\"Means against each other\")\n",
    "\n",
    "#Expected number of measurements plotted\n",
    "numOfDistances = 0;\n",
    "i = 0\n",
    "for date in dates:\n",
    "    min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "    max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "    condition = DH_plus_dist.sample_date==date\n",
    "\n",
    "    axis[i].set_xlim(min,max)\n",
    "\n",
    "    print(dates)\n",
    "    axis[i].plot(DH_plus_dist.index,DH_plus_dist.loc[:,'distance'])\n",
    "\n",
    "    axis[i].plot(DH_rolling_window_summary.loc[:,'three_record_midpoint'],DH_rolling_window_summary.loc[:,'three_record_mean'])\n",
    "    axis[i].plot(DH_rolling_window_summary.loc[:,'five_record_midpoint'],DH_rolling_window_summary.loc[:,'five_record_mean'])\n",
    "    axis[i].plot(DH_rolling_window_summary.loc[:,'ten_record_midpoint'],DH_rolling_window_summary.loc[:,'ten_record_mean']) \n",
    "    #Each subplot adds to total number of values plotted\n",
    "    numOfDistances = numOfDistances + DH_rolling_window_summary.loc[condition,'three_record_mean'].count()\n",
    "    #Checks to see if all values are plotted.\n",
    "    print(numOfDistances)\n",
    "    i+=1\n",
    "figure.legend([\"distance\",\"three_record_mean\",\"five_record_mean\",\"ten_record_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574c89e-2107-44ed-93f2-ab31619cad6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = DH_plus_dist.loc[:,'sample_date'].unique()\n",
    "\n",
    "figure_one, axis_one = plt.subplots(dates.size)\n",
    "figure_two, axis_two = plt.subplots(dates.size)\n",
    "figure_three, axis_three = plt.subplots(dates.size)\n",
    "\n",
    "figure_one.suptitle(\"Mean per every three records\")\n",
    "figure_two.suptitle(\"Mean per every five records\")\n",
    "figure_three.suptitle(\"Mean per every ten records\")\n",
    "\n",
    "#Expected number of measurements plotted\n",
    "numOfDistances = 0;\n",
    "i = 0\n",
    "for date in dates:\n",
    "    min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "    max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "    condition = DH_plus_dist.sample_date==date\n",
    "\n",
    "    axis_one[i].set_xlim(min,max)\n",
    "    axis_two[i].set_xlim(min,max)\n",
    "    axis_three[i].set_xlim(min,max)\n",
    "    #axis_four[i].set_xlim(min,max)\n",
    "\n",
    "    print(dates)\n",
    "\n",
    "    axis_one[i].plot(DH_plus_dist.index,DH_plus_dist.loc[:,'distance'])\n",
    "    axis_one[i].plot(DH_rolling_window_summary.loc[:,'three_record_midpoint'],DH_rolling_window_summary.loc[:,'three_record_mean'])\n",
    "    axis_two[i].plot(DH_plus_dist.index,DH_plus_dist.loc[:,'distance'])\n",
    "    axis_two[i].plot(DH_rolling_window_summary.loc[:,'five_record_midpoint'],DH_rolling_window_summary.loc[:,'five_record_mean'])\n",
    "    axis_three[i].plot(DH_plus_dist.index,DH_plus_dist.loc[:,'distance'])\n",
    "    axis_three[i].plot(DH_rolling_window_summary.loc[:,'ten_record_midpoint'],DH_rolling_window_summary.loc[:,'ten_record_mean']) \n",
    "    #Each subplot adds to total number of values plotted\n",
    "    numOfDistances = numOfDistances + DH_rolling_window_summary.loc[condition,'three_record_mean'].count()\n",
    "    #Checks to see if all values are plotted.\n",
    "    print(numOfDistances)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64c986-cb0a-4ecc-83fb-9c3f9170215a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### First test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33d12d-c84e-440f-8cd5-cdd01cf02ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of test_dataset.\n",
    "\n",
    "DH_rolling_window_summary = statistical_summary(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f2295-a5c2-44d8-9703-774a50886002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_rolling_window_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5308f-1e14-426a-9bff-032161fd6764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17206f86-96ae-450c-9433-6a243daa7370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following DH_plus_dist windowing methods.\n",
    "DH_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27693fb9-734b-4305-90b6-0ade89e23a96",
   "metadata": {},
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5238b-2460-41c5-a7f7-b19d68aa13fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d84fca-bdd9-498d-bd22-7dd4fb39b51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cacd6-9f13-4ea1-b68b-365ef932506f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7eb51-4f71-4b73-82e6-02a817b01dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b00a7-6bf7-4c74-a438-65cfb8c05ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29173884-79db-4d50-8422-b9ea53d7282f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd416417-3166-43ed-b7d0-8580cf164759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581164b9-5259-4e01-8e9f-fd135ef321eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02549c-abce-45d6-b4d2-4fd9c3170610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac0241-2dae-4c09-a3b9-bc9d275edf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bc762-86b9-4552-84e0-7752a330ba45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(DH_plus_dist, DH_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36ac02-09a0-4b23-a894-4821d6ac3368",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Second test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a295319-2155-48f0-b7d7-79902b086c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of test_dataset.\n",
    "\n",
    "LA_rolling_window_summary = statistical_summary(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46beca0f-9d0b-456f-82d8-619131ba86f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc154a-2a81-409d-895d-58384e418f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following LA_plus_dist windowing methods.\n",
    "LA_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4533839-5dbf-4aa2-97bb-ff744cc707f1",
   "metadata": {},
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658113dd-0378-4cf9-b178-a5157c357d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30102ec-c6b6-41b9-8ccb-8efe97fec071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc9a52-7a3e-4115-bbfd-a7bfe2c8def4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351979d-3332-4c52-a149-25572a9fff82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2e019-b57f-4632-95cf-0bd11e5163fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd13f1-a93b-4533-ba72-8dc2524ee408",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd8f1f-a440-4335-8c0c-cafeeff18ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fec13-5977-4b36-9d9b-31d3f24a1a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412df695-3468-4750-a180-08ee0a7f4bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ec759-8b85-4546-a59c-8a2769d52675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede7df5-e875-493d-a2e4-059545ac7e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(LA_plus_dist, LA_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b98a7-8039-499a-96fe-6542aaf20b12",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Third test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323fb9e-b12e-4277-ac78-3d629847c3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DataFrame of statistical summaries of test_dataset.\n",
    "\n",
    "Lucy_rolling_window_summary = statistical_summary(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd438534-8a30-45b7-97e4-52330acec8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_rolling_window_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4a875-a846-4517-b9dc-e3f6cb1c35c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lucy_rolling_window_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e62b33-f558-4f21-8654-d8c4486cdc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expeced number of values which are not null or nan for the following DH_plus_dist windowing methods.\n",
    "Lucy_rolling_window_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a998f-9111-4b02-b2ef-58d8b3609210",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### All on seperate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911e55c-6598-4b78-a854-8e552e90545b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7533eb-5afc-402c-9411-e3161e9edaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe6512-5ea0-4b70-8afc-dc7b0ee117c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02cb54-8b02-48d6-9335-4f591783962a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19c5aa-c568-4ee9-8d5f-548dfc86d6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_separate(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced1604-b173-48bb-969e-351ce04cefaf",
   "metadata": {},
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816a34a-0e8b-4f11-b37b-0eb73df1df80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475c91-906d-4ade-88e7-f41168065617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fractional_changes_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203da16f-cbe2-4a99-8b2e-e01dc05629fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e25cb-4c0b-4176-b0c9-72923109b97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_deviations_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e13f0-8edd-439a-bc3a-47f24b1a111a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances_combined(Lucy_16_20_May_plus_dist, Lucy_rolling_window_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe0a78-ad18-4567-8c43-ade2fdb653f2",
   "metadata": {},
   "source": [
    "### Indexing, multiindexing and hourly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae707f-4e70-42c3-8601-21da4cb7658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.read_csv(\"DH_plus_dist_interpolated.csv\")\n",
    "example = example.set_index(pd.to_datetime(example['sample_date']+\" \"+example['sample_time'], dayfirst=True))\n",
    "example.loc[:,'hour_average'] = 0\n",
    "\n",
    "# Multilevel index, an array of tuples with 2 values the date and hour number from 0 to 23.\n",
    "\n",
    "# transform() method so hourly averages can be added to each index of the original DataFrame instead of adding them to new idexes based on the groupby parameters/criteria.\n",
    "# Original and grouped DataFrames now have the same number of records making it simple to copy hour_average column values from the grouped DataFrame to the original without any dimensionality conflict.\n",
    "example.loc[:,'hour_average'] = example.groupby([example.index.date, example.index.hour])['distance'].transform(\"mean\")\n",
    "\n",
    "# Multilevel index, an array of tuples with 2 values the date and hour number from 0 to 23.\n",
    "hour_averages = example.groupby([example.index.date, example.index.hour])['distance'].mean()\n",
    "#if example.index.date == hour_averages.index[0] and example.index.hour == hour_averages.index[0]: \n",
    "#    example.loc[:,'hour_average'] = hour_averages\n",
    "#example\n",
    "print(type(hour_averages.index))\n",
    "print(hour_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188176fc-1ee9-4e13-b947-4311a5e90ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date + hour index count:\",hour_averages.index.size)\n",
    "print(\"DateTime index count:\",example.index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00355b0d-6d6f-4266-a269-77f1c517ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3624e-1183-4c3f-afc7-bea1f0ee1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce700bf9-21fc-40ce-9c3d-32b31a7e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.index.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f48eff-5dcc-4306-bc80-21c317e437d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_averages.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18156b5b-f0ea-421c-a599-a5cf1c71b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_averages.index[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47889608-0cb6-426f-aaeb-2f3f4a995af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36013e-8ec6-46dd-af06-2428c7550f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hour = pd.read_csv(\"DH_plus_dist_interpolated.csv\")\n",
    "example_hour = example.set_index(pd.to_datetime(example_hour['sample_date']+\" \"+example_hour['sample_time'], dayfirst=True))\n",
    "example_hour = hourly_average(example_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c57752-949d-4060-abe8-a1adc8410766",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427680b-a5cc-4ee4-b7f5-5d11fde2a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hour2 = pd.read_csv(\"DH_plus_dist_interpolated.csv\")\n",
    "example_hour2 = example.set_index(pd.to_datetime(example_hour2['sample_date']+\" \"+example_hour2['sample_time'], dayfirst=True))\n",
    "example_hour2.loc[:,'hour_average'] = example_hour2.groupby([example_hour2.index.date, example_hour2.index.hour])['distance'].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5d6ca-dcb2-47a3-b696-7acb15066622",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hour2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9077e-c55b-4cbc-9d85-5a9e419182f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_hour2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f215f-3bca-4a56-b736-00e0ed11cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(example_hour2.loc[:,'hour_average']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15aa1a5-807c-489f-ae37-9294eb0837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.groupby([example.index.date, example.index.hour])['distance'].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7aa704-52b6-461b-9507-7867dccbfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_standard_deviation(test_dataset):\n",
    "    test_datasetOne = test_dataset.groupby([test_dataset.index.date, test_dataset.index.hour])['distance'].transform(\"mean\")\n",
    "    test_datasetTwo = test_dataset.groupby([test_dataset.index.date, test_dataset.index.hour]).mean()\n",
    "    #plt.plot(test_datasetOne.index,test_datasetOne.loc[:])\n",
    "    #test_datasetTwo.index = test_datasetTwo.index.set_names(['date','hour'])\n",
    "    #plt.plot(test_datasetTwo.index.get_level_values(0),test_datasetTwo.loc[:,'hour_average'])\n",
    "    e = test_dataset.resample('H').mean()\n",
    "    \n",
    "    print(pd.isna(e.loc[:,'hour_average']).sum())\n",
    "    print(pd.isna(test_dataset.loc[:,'hour_average']).sum())\n",
    "    plt.plot(e.index,e.loc[:,'hour_average'])\n",
    "    #plt.plot(test_dataset.index,test_dataset.loc[:,'hour_average'])\n",
    "    #pd.plotting.autocorrelation_plot(test_datasetOne)\n",
    "    #pd.plotting.autocorrelation_plot(test_datasetTwo)\n",
    "    #print(test_datasetTwo.index)\n",
    "    #print(test_datasetTwo)\n",
    "    print(test_dataset)\n",
    "    plt.plot(test_dataset.index,test_dataset.loc[:,'hour_average'])\n",
    "    \n",
    "    #dates = test_dataset.loc[:,'sample_date'].unique()\n",
    "    #largestY = DH_plus_dist.loc[:,'distance'].max()\n",
    "    #print(\"Largest distance value:\",largestY)\n",
    "    #print(dates)\n",
    "    #figure, axis = plt.subplots(dates.size)\n",
    "    #i = 0\n",
    "    #for date in dates:\n",
    "    #    min = pd.to_datetime(date+\" \"+\"00:00:00\", dayfirst = True)\n",
    "    #    max = pd.to_datetime(date+\" \"+\"23:59:59\", dayfirst = True)\n",
    "    #    condition = DH_plus_dist.sample_date==date\n",
    "    #    times = DH_plus_dist.loc[condition,'sample_time']\n",
    "    #    datetimes = pd.to_datetime(date+\" \"+times, dayfirst=True)\n",
    "    #    print(times)\n",
    "    #    distances = DH_plus_dist.loc[condition,'distance']\n",
    "    #    axis[i].set_xlim(min,max)\n",
    "    #    axis[i].set_ylim(0,largestY)\n",
    "    #    axis[i].plot(datetimes,distances)\n",
    "    #    # Compare against hourly average.\n",
    "    #    axis[i].plot(datetimes, DH_plus_dist.loc[condition,'hour_average'],'--')\n",
    "    #    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc2678-25f5-4961-a7e5-2f3ec9ecb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_standard_deviation(DH_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e5fc0-9c44-4555-9896-a32b5a877a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_standard_deviation(LA_plus_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072c8f0-0fa8-4a9f-9f80-eaee535c44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_standard_deviation(Lucy_16_20_May_plus_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de4046-a70a-4c56-9748-eba1d0194d3e",
   "metadata": {},
   "source": [
    "### Differences between adjacent datetimes/record interval durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74085cff-edc8-45df-83fe-2addaaf33e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "largest = 0 \n",
    "for i in range(0,DH_plus_dist.index.size-1):\n",
    "    if i == 0 :\n",
    "        diff.append(DH_plus_dist.index[i] - DH_plus_dist.index[i])\n",
    "    diff.append(DH_plus_dist.index[i+1] - DH_plus_dist.index[i])\n",
    "diff = pd.to_timedelta(diff)\n",
    "DH_plus_dist['interval_durations'] = diff\n",
    "DH_plus_dist\n",
    "plt.plot(DH_plus_dist.index,DH_plus_dist.loc[:,'interval_durations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331355e6-1f2d-4bd0-b0ea-c9db079d949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "for i in range(0,LA_plus_dist.index.size-1):\n",
    "    if i == 0 :\n",
    "        diff.append(LA_plus_dist.index[i] - LA_plus_dist.index[i])\n",
    "    diff.append(LA_plus_dist.index[i+1] - LA_plus_dist.index[i])\n",
    "diff = pd.to_timedelta(diff)\n",
    "plt.plot(LA_plus_dist.index,diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf8ded-a4ba-41ce-ab6e-f9be4a55d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "for i in range(0,Lucy_16_20_May_plus_dist.index.size-1):\n",
    "    if i == 0 :\n",
    "        diff.append(Lucy_16_20_May_plus_dist.index[i] - Lucy_16_20_May_plus_dist.index[i])\n",
    "    diff.append(Lucy_16_20_May_plus_dist.index[i+1] - Lucy_16_20_May_plus_dist.index[i])\n",
    "diff = pd.to_timedelta(diff)\n",
    "plt.plot(Lucy_16_20_May_plus_dist.index,diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9e50f-f343-40a3-a771-266ba87ceae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=LA_plus_dist.loc[(LA_plus_dist.distance>12.75) & (LA_plus_dist.distance<13.5),'distance'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a945c-0d98-44d4-b694-e524ae3615dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_plus_dist.loc[(LA_plus_dist.distance>16.75) & (LA_plus_dist.distance<18),'distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61dc0a-4f7a-49b7-9cc2-b9ac5b763126",
   "metadata": {},
   "source": [
    "### Understanding how the BaseIndexer class works and how it is used in inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54568983-6b4e-4c8f-b1b7-d8cf2e1f450a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas.api.indexers import BaseIndexer\n",
    "class CustomIndexer(BaseIndexer):\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        for i in range(num_values):\n",
    "            start[i] = i\n",
    "            end[i] = i + self.window_size\n",
    "        return start, end\n",
    "df = pd.DataFrame({\"values\": range(5)})\n",
    "\n",
    "indexer = CustomIndexer(window_size=2)\n",
    "df.rolling(window = indexer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3d267-3961-49f8-9a3c-3474cdaebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.indexers import BaseIndexer\n",
    "class CustomIndexer(BaseIndexer):\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        for i in range(num_values):\n",
    "            start[i] = i\n",
    "            end[i] = i + 3\n",
    "        return start, end\n",
    "df = pd.DataFrame({\"values\": range(5)})\n",
    "\n",
    "indexer = CustomIndexer(window_size=2)\n",
    "df.rolling(window = indexer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55981237-c5b8-4a44-a0ee-66300233d3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_expanding = [True, False, True, False, True]\n",
    "\n",
    "use_expanding\n",
    "\n",
    "df = pd.DataFrame({\"values\": range(5)})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80be861-8816-4a08-aa58-3202e13df81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomIndexer(BaseIndexer):\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        self.index_array = pd.DatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15']) # self.index\n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        for i in range(num_values):\n",
    "            start[i] = i \n",
    "            end[i] = i + 2\n",
    "            print(i)\n",
    "        print(num_values)\n",
    "        print(start,end)\n",
    "        print(self.index_array)\n",
    "        return start, end\n",
    "\n",
    "indexer = CustomIndexer()\n",
    "print(indexer.index_array)\n",
    "\n",
    "df.rolling(indexer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70e2ac-eec5-4e08-b70a-6720689f5255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas.api.indexers import BaseIndexer\n",
    "class CustomIndexer(BaseIndexer):\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        for i in range(num_values):\n",
    "            start[i] = i\n",
    "            end[i] = i + self.window_size\n",
    "        print(start,end)\n",
    "        return start, end\n",
    "df = pd.DataFrame({\"values\": range(5)})\n",
    "indexer = CustomIndexer(window_size=2)\n",
    "print(df.rolling(indexer).sum())\n",
    "\n",
    "\n",
    "\n",
    "baseIndexer = BaseIndexer()\n",
    "\n",
    "print(dir(baseIndexer))\n",
    "\n",
    "print(baseIndexer.index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc080a-b220-4271-9c93-8e3ae37aa243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas.api.indexers import BaseIndexer\n",
    "class CustomIndexer(BaseIndexer):\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        start = np.arange(num_values, dtype=np.int64)\n",
    "        end = np.arange(num_values, dtype=np.int64) + self.window_size\n",
    "        print(num_values)\n",
    "        print(start, end)\n",
    "        return start, end\n",
    "df = pd.DataFrame({\"values\": range(5)})\n",
    "indexer = CustomIndexer(window_size=2)\n",
    "df.rolling(indexer).sum()\n",
    "print(dir(indexer))\n",
    "#from pandas.api.indexers import BaseIndexer\n",
    "#class CustomIndexer(BaseIndexer):\n",
    "#    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "#        start = pd.DatetimeIndex()\n",
    "#        end = pd.DatetimeIndex()\n",
    "#        print(self.index)\n",
    "#        for i in range(num_values):\n",
    "#            print(i)\n",
    "#            start[i] = i\n",
    "#            end[i] = i\n",
    "#        print(start,end)\n",
    "#        return start, end\n",
    "\n",
    "#df = pd.DataFrame(index = pd.DatetimeIndex([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\",  \"3/1/2020 11:00:00+00:00\",  \"4/1/2020 11:00:00+00:00\",  \"5/1/2020 11:00:00+00:00\"]),data = {\"Data\" : range(5)})\n",
    "#indexer = CustomIndexer(window_size=2, index = df.index)\n",
    "#df.index\n",
    "#print(df.rolling(indexer).sum())\n",
    "#print(indexer)\n",
    "#print(pd.api.types.is_numeric_dtype(df.index))\n",
    "#print(pd.api.types.is_numeric_dtype(df['Data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af775ce3-5b66-4560-99bd-2331f17620de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas.api.indexers import BaseIndexer\n",
    "class CustomIndexer(BaseIndexer):\n",
    "    \n",
    "    #def __init__(self, index_array, window_size, **kwargs):\n",
    "        \n",
    "        #super().__init__(self,index_array, window_size, **kwargs)\n",
    "        \n",
    "    # From 1.5.0 onwards step is a mandatory parameter. Adding a default value allows for backward and foreward compatibility up to the latest version 2.2.3. Currently using 1.4.4.\n",
    "    # All rolling windows are centered reguardless of center parameter.\n",
    "    def get_window_bounds(self, num_values, min_periods, center, closed):\n",
    "        \n",
    "        start = np.empty(num_values, dtype=np.int64)\n",
    "        end = np.empty(num_values, dtype=np.int64)\n",
    "        splitDatasets = []\n",
    "        #between_duration = False\n",
    "        print(num_values)\n",
    "        print(self.window_size)\n",
    "\n",
    "        # For an even window size you can only have integer indexes. Hence \"middle\" index value is rounded up automatically to the nearest integer. Thus the \"middle\" index closer to the right value.\n",
    "        # Within the BaseIndexer class the start and end bounds work just like the range(a, b) function where: \n",
    "        # a = the starting index or value.\n",
    "        # b = the index or value up to but not including.  \n",
    "        #if self.window_size % 2 == 0:\n",
    "        #    minShift = self.window_size/2\n",
    "        #    maxShift = self.window_size/2\n",
    "        #else:\n",
    "        #    minShift = int(self.window_size/2)\n",
    "        #    maxShift = round(self.window_size/2)\n",
    "            \n",
    "        indexCut = 0\n",
    "        for i in range(num_values):\n",
    "            if self.dataset.lag.iat[i] > pd.Timedelta(15,'m'):\n",
    "                splitDatasets.append(self.dataset.iloc[indexCut:i,:])\n",
    "                indexCut = i\n",
    "            elif i == num_values-1:\n",
    "                splitDatasets.append(self.dataset.iloc[indexCut:num_values,:])\n",
    "\n",
    "        print(\"Number of split datasets:\",len(splitDatasets))\n",
    "        size = 0\n",
    "            \n",
    "        for j in splitDatasets:\n",
    "            \n",
    "            if len(j) > self.window_size:\n",
    "                \n",
    "                subwindow_size = self.window_size\n",
    "            else:\n",
    "                subwindow_size = len(j)\n",
    "            # For an even window size you can only have integer indexes. Hence \"middle\" index value is rounded up automatically to the nearest integer. Thus the \"middle\" index closer to the right value.\n",
    "            # Within the BaseIndexer class the start and end bounds work just like the range(a, b) function where: \n",
    "            # a = the starting index or value.\n",
    "            # b = the index or value up to but not including.      \n",
    "            if subwindow_size % 2 == 0:\n",
    "                minShift = subwindow_size/2\n",
    "                maxShift = subwindow_size/2\n",
    "            else:\n",
    "                minShift = int(subwindow_size/2)\n",
    "                maxShift = round(subwindow_size/2)\n",
    "                \n",
    "            for k in range(len(j)):\n",
    "                # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "                # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "                #if k - minShift < 0 and k + maxShift > len(j) - 1:\n",
    "                #    start[k + size] = 0 + size\n",
    "                #    end[k + size] = num_values + size\n",
    "                if k - minShift < 0:\n",
    "                    start[k + size] = 0 + size\n",
    "                    end[k + size] = k + maxShift + size\n",
    "                elif k + maxShift > len(j) - 1:\n",
    "                    start[k + size] = k - minShift + size\n",
    "                    end[k + size] = num_values + size\n",
    "                else:\n",
    "                    start[k + size] = k - minShift + size\n",
    "                    end[k + size] = k + maxShift + size\n",
    "            size+=len(j)\n",
    "        print(\"Number of records:\",size)\n",
    "        \n",
    "            \n",
    "        return start, end\n",
    "        \n",
    "\n",
    "        #for i in range(num_values):\n",
    "            #print(i)\n",
    "            # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "            # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "        #    if i - minShift < 0:\n",
    "        #        start[i] = 0\n",
    "        #        end[i] = i + maxShift\n",
    "                #earliest = indexes[0]\n",
    "                #latest = indexes[i+maxShift]\n",
    "        #    elif i + maxShift > num_values - 1:\n",
    "        #        start[i] = i - minShift\n",
    "        #        end[i] = num_values \n",
    "                #earliest = indexes[i-minShift]\n",
    "                #latest = indexes[indexes.size-1]\n",
    "        #    else:\n",
    "        #        start[i] = i - minShift\n",
    "        #        end[i] = i + maxShift\n",
    "        #start = pd.DatetimeIndex(start)\n",
    "        #end = pd.DatetimeIndex(end)\n",
    "        #print(start)\n",
    "        #print(self.datetimeIndexes.size)\n",
    "        #return start, end\n",
    "    \n",
    "\n",
    "        #for i in range(num_values):\n",
    "            #if self.lags[i] <= pd.Timedelta(15,'m') and between_duration == False:\n",
    "                #start = np.append(start, i)\n",
    "                #between_duration = True\n",
    "            #elif self.lags[i] > pd.Timedelta(15,'m') and between_duration == True:\n",
    "                #end = np.append(end, i)\n",
    "                #between_duration = False\n",
    "        #print(start)    \n",
    "        #return start, end\n",
    "        \n",
    "        #for j in range(minShift+1):\n",
    "        #    if self.lags[i-minShift] > pd.Timedelta(15,'m'):\n",
    "        #        start[i] = i - j\n",
    "        #        break\n",
    "                \n",
    "        #for k in range(1, maxShift+1):\n",
    "        #    if self.lags[i+maxShift] > pd.Timedelta(15,'m'):\n",
    "        #        end[i] = i + k - 1\n",
    "        #        break\n",
    "        \n",
    "        #for i in range(num_values):\n",
    "            #print(i)\n",
    "\n",
    "            # This check makes sure the earliest/lower bound DateTime is <= latest/upper bound DateTime\n",
    "            # Prevents IndexError by goind out of bounds or a ValueError due to the list traversing to the end of the list as this is how arrays work in Python.\n",
    "        #    if i - minShift < 0:\n",
    "        #        for j in range(i):\n",
    "        #            if self.lags[j] > pd.Timedelta(15,'m'):\n",
    "        #                start[i] = i - j\n",
    "        #                break\n",
    "                #start[i] = 0\n",
    "        #        for k in range(1, maxShift+1):\n",
    "        #            if self.lags[k] > pd.Timedelta(15,'m'):\n",
    "        #                end[i] = i + k - 1\n",
    "        #                break\n",
    "                #end[i] = i + maxShift\n",
    "\n",
    "        #    elif i + maxShift > num_values - 1:\n",
    "        #        for j in range(minShift+1):\n",
    "        #            if self.lags[j] > pd.Timedelta(15,'m'):\n",
    "        #                start[i] = i - j\n",
    "        #                break\n",
    "                #start[i] = i - minShift\n",
    "        #        for k in range(1, num_values - i + 1):\n",
    "        #            if self.lags[k] > pd.Timedelta(15,'m'):\n",
    "        #                end[i] = i + k - 1\n",
    "        #                break\n",
    "                #end[i] = num_values\n",
    "                \n",
    "        #    else:\n",
    "        #        for j in range(minShift+1):\n",
    "        #            if self.lags[j] > pd.Timedelta(15,'m'):\n",
    "        #                start[i] = i - j\n",
    "        #                break\n",
    "                #start[i] = i - minShift\n",
    "        #        for k in range(1, maxShift+1):\n",
    "        #            if self.lags[k] > pd.Timedelta(15,'m'):\n",
    "        #                end[i] = i + k - 1\n",
    "        #                break\n",
    "                #end[i] = i + maxShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00070374-c3cd-4453-9476-ef8048e00fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_term(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset before feature engineering:\\n\",dataset)\n",
    "    # DateTime indexes\n",
    "    dataset = dataset.set_index(pd.to_datetime(dataset['sample_date']+\" \"+dataset['sample_time'], dayfirst=True), drop = False)\n",
    "    # Makes sure records are in chronological order\n",
    "    dataset = dataset.sort_index()\n",
    "    print(dataset.describe())\n",
    "    print(\"Number of missing values:\\n\",dataset.isna().sum())\n",
    "    \n",
    "    print(\"Number of duplicate records removed:\\n\",dataset.index.duplicated().sum())\n",
    "    dataset = remove_duplicate_datetimes(dataset)\n",
    "\n",
    "    zero_distance_count = dataset.loc[dataset.distance == 0,'distance'].count()\n",
    "    print(\"Number of zero distance records removed:\\n\",zero_distance_count)\n",
    "    dataset = dataset.loc[dataset.distance > 0,:]\n",
    "    \n",
    "    dataset['lag'] = dataset.index\n",
    "    dataset['lag'] = dataset['lag'].diff()\n",
    "    print(\"Number of lags greater than 15 minutes:\", (dataset[dataset.lag > pd.Timedelta(15, \"m\")]).count())\n",
    "    \n",
    "    dataset['window_size'] = ''\n",
    "    indexCut = 0\n",
    "    sections = dataset[dataset.lag > pd.Timedelta(15, \"m\")]\n",
    "    print(\"If the first rows are identical\",sections.head(1).equals(dataset.head(1)))\n",
    "    print(sections)\n",
    "    print(dataset)\n",
    "    sections = pd.concat([dataset.head(1), sections]).drop_duplicates()\n",
    "    print(sections)\n",
    "    print(\"Number of sections:\",len(sections))\n",
    "    start = dataset.first_valid_index()\n",
    "    print(\"Head:\",start)\n",
    "    listIndex = sections.index.to_list()\n",
    "    window_size = 25\n",
    "    for i in range(len(listIndex)):\n",
    "        print(i)\n",
    "        if listIndex[i] == listIndex[-1]:\n",
    "            if len(dataset.loc[listIndex[i]:]) >= window_size:\n",
    "                dataset.loc[listIndex[i]:,'window_size'] = 25\n",
    "            else:\n",
    "                dataset.loc[listIndex[i]:,'window_size'] = len(dataset.loc[listIndex[i]:])\n",
    "        else:\n",
    "            if len(dataset.loc[listIndex[i]:listIndex[i+1]]) >= window_size:\n",
    "                dataset.loc[listIndex[i]:listIndex[i+1], 'window_size'] = 25\n",
    "            else:\n",
    "                dataset.loc[listIndex[i]:listIndex[i+1], 'window_size'] = len(dataset.loc[listIndex[i]:listIndex[i+1]])\n",
    "    print(\"Number of records less than window size 25:\",len(dataset[dataset.window_size<25]))\n",
    "        #if i.equals(sections.last_valid_index()):\n",
    "        #    dataset.loc[i:,'window_size'] = 5\n",
    "        #else:\n",
    "        #    dataset.loc[start:i,'window_size'] = 5 #len(dataset.loc[start:i]) - 1\n",
    "        #start = i\n",
    "          #dataset =          \n",
    "    #dataset.apply(,,,axis = 1)\n",
    "    #for i in range(num_values):\n",
    "    #    if dataset.lag.iat[i] > pd.Timedelta(15,'m'):\n",
    "    #        dataset['window_size'].iloc[indexCut:i] = dataset.iloc[indexCut:i,:]\n",
    "    #        #splitDatasets.append(self.dataset.iloc[indexCut:i,:])\n",
    "    #        indexCut = i\n",
    "    #    elif i == num_values-1:\n",
    "    #        dataset['window_size'].iloc[indexCut:i] = dataset.iloc[indexCut:num_values,:]\n",
    "            #splitDatasets.append(self.dataset.iloc[indexCut:num_values,:])\n",
    "    \n",
    "    custom_indexer = CustomIndexer(dataset = dataset, window_size = 25)\n",
    "    print(dir(custom_indexer))\n",
    "    twenty_five_record_mean = dataset.loc[:,'distance'].rolling(window=custom_indexer).mean()\n",
    "    twenty_five_record_standard_deviation = dataset.loc[:,'distance'].rolling(window=custom_indexer).std()\n",
    "    dataset['twenty_five_record_mean'] = twenty_five_record_mean\n",
    "    dataset['twenty_five_record_standard_deviation'] = twenty_five_record_standard_deviation\n",
    "    twenty_five_record_difference = dataset.loc[:,'distance'].rolling(25,center=True).mean().diff()\n",
    "    dataset['twenty_five_record_difference'] = twenty_five_record_difference\n",
    "    \n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"twenty_five_record_standard_deviation\"] < 0.1) | (dataset[\"distance\"].diff().abs() < 0.25) # Stationary, added additional condition regarding the amount of difference from previous value so it won't misclasify the ends of passive phases. \n",
    "    #Then I could count the number of consecutive records which meet this condition to determine whether the section is in an active or passive phase.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"] < (dataset[\"twenty_five_record_mean\"] + 1)) & (dataset[\"distance\"] > (dataset[\"twenty_five_record_mean\"] - 1)) | (dataset[\"distance\"].diff().abs() < 0.25)  # Finds noise\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.25) # Useful for retaining useful records at the ends of stationary phases.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = dataset['twenty_five_record_standard_deviation'] <= dataset[\"distance\"].std() # might also be used to support indication of transition between active and passive states.\n",
    "\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index.diff().abs() < 10)\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() < 0.1) # Alternative method to determine passive periods.\n",
    "    \n",
    "    #dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 0.25) | (dataset['twenty_five_record_standard_deviation'] <= 0.25)\n",
    "    \n",
    "    dataset['satisfied_filtering_condition'] = (dataset[\"distance\"].diff().abs() <= 1) \n",
    "    \n",
    "    subset = dataset.loc[dataset.satisfied_filtering_condition == False,:].index.to_frame(name='false_condition_interval')\n",
    "    #othersubset = subset.index.to_frame(name='false_condition_interval')\n",
    "    subset['false_condition_interval'] = subset.diff()\n",
    "    #othersubset['shifted_false_datetime'] = subset.index.to_series().diff()\n",
    "    print(subset)\n",
    "    #print(subset.index.to_series().diff())\n",
    "    #subset['shifted_false_datetime'] = pd.to_datetime(subset['sample_date']+\" \"+subset['sample_time'], dayfirst=True)\n",
    "    #subset['shifted_false_datetime'] = subset['shifted_false_datetime'].diff()\n",
    "    \n",
    "    \n",
    "    #print(\"Subset:\", subset)\n",
    "\n",
    "    dataset['shifted_datetime'] = dataset.index\n",
    "    dataset['shifted_datetime'] = dataset['shifted_datetime'].shift(1)\n",
    "    dataset = pd.concat([dataset,subset], axis=1)\n",
    "    #dataset['satisfied_filtering_condition'] = (dataset.index - dataset['shifted_datetime'] <= pd.Timedelta(15, \"m\")) # If gaps between recordings are grater than 15 minutes then the rolling can restart as long gaps between recordings can skew rolling window calculations.\n",
    "    \n",
    "    print(\"Total number of records meeting filtering condition\\n:\",dataset.loc[:,'satisfied_filtering_condition'].value_counts())\n",
    "    \n",
    "    print(\"Dataset after feature engineering:\\n\",dataset)\n",
    "    print(dataset.describe())\n",
    "\n",
    "    #relative_range_time_series_plot(dataset,'rolling_hourly_mean','rolling_hourly_standard_deviation')\n",
    "    #relative_range_time_series_plot(dataset,'difference')\n",
    "    #relative_range_time_series_plot(dataset,'ten_record_difference')\n",
    "    relative_range_time_series_plot(dataset,'twenty_five_record_mean','twenty_five_record_standard_deviation','twenty_five_record_difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e739c4-fc8a-491d-9bc4-1b0dbbe6b01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_term(\"DH_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f081f-0f01-481f-8b94-0d6a6bedfb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_term(\"LA_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed686191-e443-4ed9-88ba-4e696048afa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_term(\"Lucy_16-20May_plus_dist_interpolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6de8f-67bf-4616-a256-c8c410207195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Matplotlib version:\", mpl.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Seaborn version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b9ad4-15da-41ce-b95f-a2d322b7159e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abc = pd.DatetimeIndex([\"12/12/2024\", \"13/12/2024\", \"14/12/2024\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb0ea5-0333-4981-9289-d04d227eaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame({'B': [0, 1, 2, 3, 4, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8652e-e7ab-47b6-95c4-21083e1ef356",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.rolling(3, center = True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3257b-f511-4038-befb-0abcf3ea8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.rolling(7, center = True).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
